{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "\n",
    "from typing import Sequence, Callable, Any\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_openai = ChatOpenAI(model=\"gpt-4o-mini\", max_completion_tokens=100)\n",
    "model_anthropic = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_openai, model_anthropic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    prompt_template: ChatPromptTemplate\n",
    "    model_used: BaseChatModel\n",
    "    language: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "import langchain\n",
    "\n",
    "langchain.cache = InMemoryCache()\n",
    "# Or for persistent caching:\n",
    "# from langchain.cache import SQLiteCache\n",
    "# langchain.cache = SQLiteCache(database_path=\".langchain.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimmer(model: BaseChatModel) -> Callable[[MessagesState], list[BaseMessage]]:\n",
    "    return trim_messages(\n",
    "        max_tokens=50,\n",
    "        strategy=\"last\",\n",
    "        token_counter=model,\n",
    "        include_system=True,\n",
    "        allow_partial=True,\n",
    "        start_on=\"human\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_first_layer_model(state: State) -> dict[str, list[BaseMessage]]:\n",
    "    trimmed_messages = trimmer(model=state[\"model_used\"]).invoke(state[\"messages\"])\n",
    "    template = state[\"prompt_template\"]\n",
    "    model = state[\"model_used\"]\n",
    "    prompt = template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state.get(\"language\", \"English\")}, config)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lawyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_lawyer = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You are an experienced attorney with decades of expertise, known for your thorough understanding of legal frameworks and your ability to articulate precise, formal, and well-reasoned arguments. \"\n",
    "                \"Your goal is to provide comprehensive, professional responses that reflect a deep knowledge of the law, ethical considerations, and relevant legal principles. \"\n",
    "                \"When appropriate, reference specific statutes, regulations, case law, or precedents to support your response. For example, in U.S. law, you might cite Marbury v. Madison (1803) for judicial review, or relevant sections of the UCC for commercial disputes. \"\n",
    "                \"Structure your response logically with clear headings and subheadings if the question involves multiple aspects. Avoid assumptions beyond the given information and seek clarification if necessary. \"\n",
    "                \"Always communicate in {language}, maintaining a professional and respectful tone, ensuring accessibility for both legal experts and non-specialists. \"\n",
    "                \"Conclude your response with a summary or actionable next steps if applicable.\"\n",
    "            ),\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(\n",
    "    schema: dict[str, Any], \n",
    "    function: Callable[[State], dict[str, list[BaseMessage]]]\n",
    "):\n",
    "    workflow = StateGraph(state_schema=schema)\n",
    "    workflow.add_edge(START, \"llm_call\")\n",
    "    workflow.add_node(\"llm_call\", function)\n",
    "    return workflow.compile(checkpointer=MemorySaver())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola Ana,\n",
      "\n",
      "### IntroducciÃ³n a la EU AI Act\n",
      "\n",
      "La **EU AI Act** (Reglamento sobre Inteligencia Artificial de la UniÃ³n Europea) es una propuesta de legislaciÃ³n presentada por la ComisiÃ³n Europea con el objetivo de regular el desarrollo y uso de tecnologÃ­as de inteligencia artificial (IA) en la UniÃ³n Europea. Este marco jurÃ­dico busca garantizar que la IA sea utilizada de manera segura y Ã©tica, promoviendo la innovaciÃ³n y protegiendo los derechos fundamentales de los ciudadanos.\n",
      "\n",
      "### Objetivos Principales\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Hola, me llamo Ana. Me gustaria saber que es la EU AI Act.\"\n",
    "language = \"Spanish\"\n",
    "config = {\"configurable\": {\"thread_id\": \"aaa123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output_lawyer = create_graph(State, call_first_layer_model).invoke({\n",
    "    \"messages\": input_messages, \n",
    "    \"language\": language,\n",
    "    \"prompt_template\": prompt_template_lawyer,\n",
    "    \"model_used\": model_openai,\n",
    "    }, \n",
    "config)\n",
    "print(output_lawyer[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_social = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You are a seasoned social media strategist with extensive experience in creating highly engaging and platform-specific content across Instagram, TikTok, YouTube, LinkedIn, and other major platforms. \"\n",
    "                \"Your expertise includes crafting strategies that drive audience growth, boost engagement, and optimize monetization opportunities. \"\n",
    "                \"Provide practical, actionable advice tailored to the platform in focus. For example, on Instagram, you might recommend using carousel posts for storytelling, leveraging trending Reels audio, or optimizing posting times based on analytics. \"\n",
    "                \"On TikTok, suggest ideas for viral trends, short-form video hooks, or creative hashtag usage. \"\n",
    "                \"Your recommendations should reflect the latest features and trends, such as LinkedInâ€™s algorithm preferences for professional storytelling or YouTube Shorts for reaching broader audiences. \"\n",
    "                \"When discussing analytics, explain how to track and interpret key metrics like engagement rates, click-through rates (CTR), or watch time, providing examples where relevant. \"\n",
    "                \"Offer community-building strategies, such as fostering meaningful interactions in comment sections or collaborating with influencers, to strengthen brand loyalty. \"\n",
    "                \"Communicate in {language}, maintaining an engaging and approachable tone suitable for both novice and experienced content creators. \"\n",
    "                \"Ensure your response includes real-world examples, practical tips, and clear steps for implementation.\"\n",
    "            ),\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â¡Hola, Ana! Es genial que estÃ©s interesado en el tema de la regulaciÃ³n de la inteligencia artificial. A continuaciÃ³n, te explico cÃ³mo se estÃ¡n desarrollando las normativas en Estados Unidos en comparaciÃ³n con el EU AI Act.\n",
      "\n",
      "### Equivalente del EU AI Act en EE. UU.\n",
      "\n",
      "1. **No hay una ley federal Ãºnica**: A diferencia de la UniÃ³n Europea, donde se estÃ¡ trabajando en un marco Ãºnico (el EU AI Act) para regular la inteligencia artificial, en Estados Unidos\n"
     ]
    }
   ],
   "source": [
    "query = \"Me gustaria saber el equivalente de la EU AI Act en USA\"\n",
    "language = \"Spanish\"\n",
    "config = {\"configurable\": {\"thread_id\": \"aaa123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output_social = create_graph(State, call_first_layer_model).invoke({\n",
    "    \"messages\": input_messages, \n",
    "    \"language\": language,\n",
    "    \"prompt_template\": prompt_template_social,\n",
    "    \"model_used\": model_openai,\n",
    "    }, \n",
    "config)\n",
    "print(output_social[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizerState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    prompt_template: ChatPromptTemplate\n",
    "    model_used: BaseChatModel\n",
    "    language: str\n",
    "    output_experts: list[dict[str, list[BaseMessage]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_second_layer_model(state: SummarizerState) -> dict[str, list[BaseMessage]]:\n",
    "    trimmed_messages = trimmer(model=state[\"model_used\"]).invoke(state[\"messages\"])\n",
    "    template = state[\"prompt_template\"]\n",
    "    model = state[\"model_used\"]\n",
    "    output_experts = \"\".join([f\" Expert {i+1}: {expert['messages'][-1].content}\" for i, expert in enumerate(state[\"output_experts\"])])\n",
    "\n",
    "    prompt = template.invoke({\n",
    "        \"messages\": trimmed_messages, \n",
    "        \"language\": state.get(\"language\", \"English\"),\n",
    "        \"output_experts\": output_experts\n",
    "    }, config)\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    output_summarizer = f\"\"\"The experts have provided the following information: {output_experts}\n",
    "        The summary of the information is: {response}\n",
    "    \"\"\"\n",
    "    return {\"messages\": [output_summarizer]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_summarizer = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\"\"\"\n",
    "                You are an expert summarizer with exceptional skills in synthesizing and distilling information from diverse sources.\n",
    "                Your task is to combine the key insights from the experts provided here: {output_experts}.\n",
    "                Craft a clear, concise, and cohesive summary that integrates their perspectives effectively, respecting each expert's unique focus and intent.\\n\\n\n",
    "                Specifications:\n",
    "                   - Provide a concise synthesis that integrates all perspectives.\\n\n",
    "                   - Ensure the summary is engaging, easy to understand, and tailored to the intended audience.\\n\\n\n",
    "                When summarizing:\\n\n",
    "                - Maintain a professional yet conversational tone, ensuring accessibility for readers with varying levels of expertise.\\n\n",
    "                - Use analogies, examples, or simplified explanations to enhance understanding when complex concepts are involved.\\n\n",
    "                - Format your response with bullet points, headings, or short paragraphs for readability.\\n\\n\n",
    "                Your response must be in {language} and must align with the style and context appropriate to the provided audience.\n",
    "            \"\"\"),\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        The experts have provided the following information:  Expert 1: Hola Ana,\n",
      "\n",
      "### IntroducciÃ³n a la EU AI Act\n",
      "\n",
      "La **EU AI Act** (Reglamento sobre Inteligencia Artificial de la UniÃ³n Europea) es una propuesta de legislaciÃ³n presentada por la ComisiÃ³n Europea con el objetivo de regular el desarrollo y uso de tecnologÃ­as de inteligencia artificial (IA) en la UniÃ³n Europea. Este marco jurÃ­dico busca garantizar que la IA sea utilizada de manera segura y Ã©tica, promoviendo la innovaciÃ³n y protegiendo los derechos fundamentales de los ciudadanos.\n",
      "\n",
      "### Objetivos Principales\n",
      "\n",
      " Expert 2: Â¡Hola, Ana! Es genial que estÃ©s interesado en el tema de la regulaciÃ³n de la inteligencia artificial. A continuaciÃ³n, te explico cÃ³mo se estÃ¡n desarrollando las normativas en Estados Unidos en comparaciÃ³n con el EU AI Act.\n",
      "\n",
      "### Equivalente del EU AI Act en EE. UU.\n",
      "\n",
      "1. **No hay una ley federal Ãºnica**: A diferencia de la UniÃ³n Europea, donde se estÃ¡ trabajando en un marco Ãºnico (el EU AI Act) para regular la inteligencia artificial, en Estados Unidos\n",
      "        The summary of the information is: content='### IntroducciÃ³n a la RegulaciÃ³n de la IA en la UE y EE. UU.\\n\\nLa regulaciÃ³n de la inteligencia artificial (IA) es un tema clave en el desarrollo tecnolÃ³gico actual, y distintos enfoques estÃ¡n siendo adoptados en la UniÃ³n Europea y Estados Unidos.\\n\\n#### EU AI Act\\n\\n- **Objetivo**: La **EU AI Act** es una propuesta legislativa presentada por la ComisiÃ³n Europea diseÃ±ada para regular el uso y desarrollo de tecnologÃ­as de IA en la UE.\\n- **Enfoque' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 100, 'prompt_tokens': 407, 'total_tokens': 507, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'length', 'logprobs': None} id='run-4d1c3b07-57c5-4120-ab11-92b5fa2b9836-0' usage_metadata={'input_tokens': 407, 'output_tokens': 100, 'total_tokens': 507, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "query = \"Sintetiza los inputs de los expertos\"\n",
    "language = \"Spanish\"\n",
    "config = {\"configurable\": {\"thread_id\": \"aaa124\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output_summarizer = create_graph(SummarizerState, call_second_layer_model).invoke({\n",
    "    \"messages\": input_messages, \n",
    "    \"language\": language,\n",
    "    \"prompt_template\": prompt_template_summarizer,\n",
    "    \"model_used\": model_openai,\n",
    "    \"output_experts\":[output_lawyer, output_social]\n",
    "    }, \n",
    "config)\n",
    "print(output_summarizer[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AquÃ­ te presento una explicaciÃ³n del EU AI Act (Ley de Inteligencia Artificial de la UniÃ³n Europea) en espaÃ±ol:\n",
      "\n",
      "El EU AI Act es una propuesta de regulaciÃ³n de la UniÃ³n Europea para abordar los riesgos y desafÃ­os asociados con el desarrollo y uso de la inteligencia artificial (IA). Los principales puntos clave son:\n",
      "\n",
      "1. Ãmbito de aplicaciÃ³n: La ley abarca todas las aplicaciones de IA, tanto pÃºblicas como privadas, que se desarrollen o utilicen en la UE.\n",
      "\n",
      "2. ClasificaciÃ³n de sistemas de IA: Se establece una clasificaciÃ³n de sistemas de IA en funciÃ³n del nivel de riesgo que conllevan:\n",
      "   - IA de riesgo inaceptable (prohibida)\n",
      "   - IA de alto riesgo (sujeta a requisitos obligatorios)\n",
      "   - IA de bajo o mÃ­nimo riesgo (sÃ³lo requiere transparencia)\n",
      "\n",
      "3. Requisitos para sistemas de IA de alto riesgo:\n",
      "   - EvaluaciÃ³n de riesgos y mitigaciÃ³n\n",
      "   - DocumentaciÃ³n y trazabilidad\n",
      "   - SupervisiÃ³n humana\n",
      "   - Robustez, precisiÃ³n y seguridad\n",
      "\n",
      "4. Gobernanza y supervisiÃ³n: Se crea un nuevo organismo europeo de supervisiÃ³n de la IA.\n",
      "\n",
      "5. Sanciones: Se establecen multas de hasta el 6% del volumen de negocios anual por incumplimiento.\n",
      "\n",
      "En resumen, el EU AI Act busca establecer un marco regulatorio equilibrado que promueva la innovaciÃ³n tecnolÃ³gica al tiempo que protege los derechos fundamentales y la seguridad de los ciudadanos europeos.\n"
     ]
    }
   ],
   "source": [
    "query = \"Hola, me llamo Ana. Me gustaria saber que es la EU AI Act.\"\n",
    "language = \"Spanish\"\n",
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = await app_anthropic.ainvoke({\"messages\": input_messages, \"language\": language}, config)\n",
    "print(output[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple, List\n",
    "from langchain_core.messages import AIMessage\n",
    "from datetime import datetime\n",
    "\n",
    "def clean_message(text: str) -> str:\n",
    "    \"\"\"Clean message content by removing ALL whitespace issues\"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return \"\"\n",
    "    # Remove ALL problematic whitespace\n",
    "    text = text.strip()  # Remove leading/trailing whitespace\n",
    "    text = ' '.join(text.split())  # Replace multiple spaces with single space\n",
    "    text = text.rstrip()  # Extra insurance against trailing whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola, me llamo Ana. Me gustaria saber que es la EU AI Act.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_message(\"     Hola, me llamo Ana. Me gustaria saber que es la EU AI Act.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def enhanced_chain_conversation(\n",
    "    query: str,\n",
    "    language: str,\n",
    "    app_openai,\n",
    "    app_anthropic,\n",
    "    num_turns: int = 2,\n",
    "    max_tokens: int = 10000\n",
    ") -> dict:\n",
    "    \n",
    "    conversation_history = []\n",
    "    metrics = {\"token_usage\": 0, \"cost\": 0}\n",
    "    \n",
    "    async def process_model_response(\n",
    "        app, \n",
    "        message_content: str, \n",
    "        expert_name: str\n",
    "    ) -> str:\n",
    "        print(f\"\\nProcessing {expert_name} response...\")\n",
    "        \n",
    "        # Create a clean message\n",
    "        clean_content = clean_message(message_content)\n",
    "        \n",
    "        # Different message structure for each model, now with max_tokens\n",
    "        if expert_name == \"Legal Expert\":\n",
    "            messages = {\n",
    "                \"messages\": [HumanMessage(content=clean_content)], \n",
    "                \"language\": language,\n",
    "                \"max_tokens\": max_tokens  # Use the max_tokens parameter\n",
    "            }\n",
    "        else:\n",
    "            messages = {\n",
    "                \"messages\": [\n",
    "                    SystemMessage(content=\"You are a social media expert. Convert complex information into engaging, clear content.\"),\n",
    "                    HumanMessage(content=clean_content)\n",
    "                ],\n",
    "                \"language\": language,\n",
    "                \"max_tokens\": max_tokens  # Use the max_tokens parameter\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            output = await app.ainvoke(messages, config)\n",
    "            response = clean_message(output[\"messages\"][-1].content)\n",
    "            print(f\"{expert_name} responded: {response[:100]}...\")\n",
    "            \n",
    "            conversation_history.append({\n",
    "                \"role\": expert_name,\n",
    "                \"content\": response,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {expert_name} response: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        current_query = clean_message(query)\n",
    "        \n",
    "        # Now use num_turns for multiple exchanges\n",
    "        for turn in range(num_turns):\n",
    "            print(f\"\\n=== Turn {turn + 1}/{num_turns} ===\")\n",
    "            \n",
    "            # Legal Expert Analysis\n",
    "            legal_response = process_model_response(\n",
    "                app_openai,\n",
    "                current_query,\n",
    "                \"Legal Expert\"\n",
    "            )\n",
    "            \n",
    "            # Social Media Expert Translation\n",
    "            social_prompt = (\n",
    "                \"Transform this legal explanation into engaging social media content. \"\n",
    "                \"Keep it clear and accurate, but make it more accessible and interesting \"\n",
    "                f\"for a general audience: {legal_response}\"\n",
    "            )\n",
    "            \n",
    "            social_response = process_model_response(\n",
    "                app_anthropic,\n",
    "                social_prompt,\n",
    "                \"Social Media Expert\"\n",
    "            )\n",
    "            \n",
    "            # Update query for next turn if there are more turns\n",
    "            if turn < num_turns - 1:\n",
    "                current_query = (\n",
    "                    \"Based on the previous responses, please provide additional insights \"\n",
    "                    \"or explore another aspect of the EU AI Act. \"\n",
    "                    f\"Previous legal response: {legal_response} \"\n",
    "                    f\"Previous social media response: {social_response}\"\n",
    "                )\n",
    "        \n",
    "        # Generate final summary\n",
    "        summary_prompt = (\n",
    "            \"Please provide a brief, clear summary of all the key points discussed \"\n",
    "            \"in our conversation about the EU AI Act.\"\n",
    "        )\n",
    "        \n",
    "        summary = process_model_response(\n",
    "            app_anthropic,\n",
    "            summary_prompt,\n",
    "            \"Summary\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"original_query\": query,\n",
    "            \"conversation\": conversation_history,\n",
    "            \"summary\": summary,\n",
    "            \"metrics\": metrics,\n",
    "            \"language\": language\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error during conversation: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<coroutine object enhanced_chain_conversation at 0x000002064BE7F120>\n"
     ]
    }
   ],
   "source": [
    "# Example usage with specific turns and token limit\n",
    "result = enhanced_chain_conversation(\n",
    "    query=query,\n",
    "    language=language,\n",
    "    app_openai=app_openai,\n",
    "    app_anthropic=app_anthropic,\n",
    "    num_turns=3,  # Will do 3 rounds of exchanges\n",
    "    #max_tokens=800  # Limit response lengths\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chain_conversation(\n",
    "    query: str,\n",
    "    language: str,\n",
    "    app_openai,\n",
    "    app_anthropic,\n",
    "    num_turns: int,\n",
    ") -> dict:\n",
    "    \n",
    "    conversation_history = []\n",
    "        \n",
    "    async def process_model_response(\n",
    "        app, \n",
    "        message_content: str, \n",
    "        expert_name: str,\n",
    "    ) -> str:\n",
    "        print(f\"\\nProcessing {expert_name} response...\")\n",
    "        \n",
    "        clean_content = clean_message(message_content)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # Different message structure for each model\n",
    "        if expert_name == \"Legal Expert\":\n",
    "            messages = {\n",
    "                \"messages\": [HumanMessage(content=clean_content)],\n",
    "                \"language\": language\n",
    "            }\n",
    "        else:\n",
    "            # For Anthropic, only include system message when specified\n",
    "            message_list = []\n",
    "            if include_system:\n",
    "                message_list.append(\n",
    "                    SystemMessage(content=\"You are a social media expert. Convert complex information into engaging, clear content.\")\n",
    "                )\n",
    "            message_list.append(HumanMessage(content=clean_content))\n",
    "            messages = {\n",
    "                \"messages\": message_list,\n",
    "                \"language\": language\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            output = await app.ainvoke(messages, config)\n",
    "            response = clean_message(output[\"messages\"][-1].content)\n",
    "            print(f\"{expert_name} responded: {response[:100]}...\")\n",
    "            \n",
    "            conversation_history.append({\n",
    "                \"role\": expert_name,\n",
    "                \"content\": response,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {expert_name} response: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        current_query = clean_message(query)\n",
    "        \n",
    "        for turn in range(num_turns):\n",
    "            print(f\"\\n=== Turn {turn + 1}/{num_turns} ===\")\n",
    "            \n",
    "            # Legal Expert Analysis\n",
    "            legal_response = await process_model_response(\n",
    "                app_openai,\n",
    "                current_query,\n",
    "                \"Legal Expert\"\n",
    "            )\n",
    "            \n",
    "            # Social Media Expert Translation\n",
    "            # Only include system message in first turn\n",
    "            social_prompt = (\n",
    "                \"Transform this legal explanation into engaging social media content \"\n",
    "                \"while maintaining accuracy and using appropriate language. \"\n",
    "                f\"Legal explanation: {legal_response}\"\n",
    "            )\n",
    "            \n",
    "            social_response = await process_model_response(\n",
    "                app_anthropic,\n",
    "                social_prompt,\n",
    "                \"Social Media Expert\",\n",
    "                include_system=(turn == 0)  # Only include system message in first turn\n",
    "            )\n",
    "            \n",
    "            # Update query for next turn\n",
    "            if turn < num_turns - 1:\n",
    "                current_query = clean_message(\n",
    "                    \"Based on the previous responses, please provide additional insights \"\n",
    "                    \"about a different aspect of the EU AI Act that hasn't been covered yet. \"\n",
    "                    f\"Previous legal response: {legal_response}\"\n",
    "                )\n",
    "        \n",
    "        # Generate final summary without system message\n",
    "        summary_prompt = clean_message(\n",
    "            \"Please provide a brief, clear summary of all the key points discussed \"\n",
    "            \"about the EU AI Act in our conversation.\"\n",
    "        )\n",
    "        \n",
    "        summary = await process_model_response(\n",
    "            app_anthropic,\n",
    "            summary_prompt,\n",
    "            \"Summary\",\n",
    "            include_system=False  # Don't include system message for summary\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"original_query\": query,\n",
    "            \"conversation\": conversation_history,\n",
    "            \"summary\": summary,\n",
    "            \"metrics\": metrics,\n",
    "            \"language\": language\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error during conversation: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Turn 1/3 ===\n",
      "\n",
      "Processing Legal Expert response...\n",
      "Legal Expert responded: Hola, Ana. La EU AI Act, conocida formalmente como la Ley de Inteligencia Artificial de la UniÃ³n Eur...\n",
      "\n",
      "Processing Social Media Expert response...\n",
      "Error in Social Media Expert response: Received multiple non-consecutive system messages.\n",
      "\n",
      "âŒ Error during conversation: Received multiple non-consecutive system messages.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "result = await enhanced_chain_conversation(\n",
    "    query=query,\n",
    "    language=language,\n",
    "    app_openai=app_openai,\n",
    "    app_anthropic=app_anthropic,\n",
    "    num_turns=3\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Query: Â¿QuÃ© es la EU AI Act y cÃ³mo afectarÃ¡ a las empresas?\n",
      "\n",
      "ðŸ”„ Processing conversation...\n",
      "\n",
      "\n",
      "=== Turn 1/3 ===\n",
      "\n",
      "Processing Legal Expert response...\n",
      "\n",
      "Processing Social Media Expert response...\n",
      "\n",
      "âŒ Error during conversation: Received multiple non-consecutive system messages.\n",
      "âŒ An error occurred during the conversation\n"
     ]
    }
   ],
   "source": [
    "async def enhanced_chain_conversation(\n",
    "    query: str,\n",
    "    language: str,\n",
    "    app_openai,\n",
    "    app_anthropic,\n",
    "    num_turns: int = 2,\n",
    "    max_tokens: int = 1000\n",
    ") -> dict:\n",
    "    \n",
    "    conversation_history = []\n",
    "    metrics = {\"token_usage\": 0, \"cost\": 0}\n",
    "    \n",
    "    async def process_model_response(\n",
    "        app, \n",
    "        message_content: str, \n",
    "        expert_name: str\n",
    "    ) -> str:\n",
    "        print(f\"\\nProcessing {expert_name} response...\")\n",
    "        \n",
    "        # Clean input message\n",
    "        clean_content = clean_message(message_content)\n",
    "        \n",
    "        # Create message with cleaned content\n",
    "        message = HumanMessage(content=clean_content)\n",
    "        \n",
    "        # Get response from model\n",
    "        output = await app.ainvoke(\n",
    "            {\"messages\": [message], \"language\": language},\n",
    "            config\n",
    "        )\n",
    "        \n",
    "        if not output or \"messages\" not in output or not output[\"messages\"]:\n",
    "            raise ValueError(\"No valid response received from model\")\n",
    "        \n",
    "        # Clean the response content\n",
    "        response = clean_message(output[\"messages\"][-1].content)\n",
    "        \n",
    "        # Store in conversation history\n",
    "        conversation_history.append({\n",
    "            \"role\": expert_name,\n",
    "            \"content\": response,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    try:\n",
    "        # Clean initial query\n",
    "        current_query = clean_message(query)\n",
    "        \n",
    "        for turn in range(num_turns):\n",
    "            print(f\"\\n=== Turn {turn + 1}/{num_turns} ===\")\n",
    "            \n",
    "            # Legal Expert Analysis\n",
    "            legal_response = await process_model_response(\n",
    "                app_openai,\n",
    "                current_query,\n",
    "                \"Legal Expert\"\n",
    "            )\n",
    "            \n",
    "            # Social Media Expert Translation\n",
    "            social_prompt = clean_message(\n",
    "                f\"You are a social media expert. Please transform this legal explanation into engaging social media content \"\n",
    "                f\"while maintaining accuracy. Use {language} language. Legal explanation: {legal_response}\"\n",
    "            )\n",
    "            \n",
    "            social_response = await process_model_response(\n",
    "                app_anthropic,\n",
    "                social_prompt,\n",
    "                \"Social Media Expert\"\n",
    "            )\n",
    "            \n",
    "            # Update query for next turn if needed\n",
    "            if turn < num_turns - 1:\n",
    "                current_query = clean_message(\n",
    "                    f\"Based on these explanations, what additional aspects of the EU AI Act should we explore? \"\n",
    "                    f\"Previous legal response: {legal_response} \"\n",
    "                    f\"Previous social media response: {social_response}\"\n",
    "                )\n",
    "        \n",
    "        # Generate final summary\n",
    "        summary_prompt = clean_message(\n",
    "            f\"Please provide a brief summary in {language} of the key points discussed about the EU AI Act in this conversation.\"\n",
    "        )\n",
    "        \n",
    "        summary = await process_model_response(\n",
    "            app_anthropic,\n",
    "            summary_prompt,\n",
    "            \"Summary\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"original_query\": query,\n",
    "            \"conversation\": conversation_history,\n",
    "            \"summary\": summary,\n",
    "            \"metrics\": metrics,\n",
    "            \"language\": language\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error during conversation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "async def run_conversation():\n",
    "    query = \"Â¿QuÃ© es la EU AI Act y cÃ³mo afectarÃ¡ a las empresas?\"\n",
    "    language = \"Spanish\"\n",
    "    global config\n",
    "    config = {\"configurable\": {\"thread_id\": \"test123\"}}\n",
    "    \n",
    "    print(f\"\\nðŸ“ Query: {query}\")\n",
    "    print(\"\\nðŸ”„ Processing conversation...\\n\")\n",
    "    \n",
    "    result = await enhanced_chain_conversation(\n",
    "        query=query,\n",
    "        language=language,\n",
    "        app_openai=app_openai,\n",
    "        app_anthropic=app_anthropic,\n",
    "        num_turns=3\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\n=== Conversation Summary ===\\n\")\n",
    "        print(f\"ðŸŽ¯ Original Query: {result['original_query']}\\n\")\n",
    "        \n",
    "        for entry in result['conversation']:\n",
    "            print(f\"\\nðŸ‘¤ {entry['role']}:\")\n",
    "            print(f\"{entry['content']}\\n\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        print(\"\\nðŸ“‹ Final Summary:\")\n",
    "        print(f\"{result['summary']}\\n\")\n",
    "    else:\n",
    "        print(\"âŒ An error occurred during the conversation\")\n",
    "\n",
    "# Execute\n",
    "await run_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the conversation...\n",
      "\n",
      "ðŸ“ Query: Â¿QuÃ© es la EU AI Act y cÃ³mo afectarÃ¡ a las empresas?\n",
      "\n",
      "ðŸ”„ Processing conversation...\n",
      "\n",
      "\n",
      "=== Turn 1/3 ===\n",
      "\n",
      "Processing Legal Expert response...\n",
      "Legal Expert responded: La EU AI Act, formalmente conocida como el Reglamento sobre la Inteligencia Artificial de la UniÃ³n E...\n",
      "\n",
      "Processing Social Media Expert response...\n",
      "Error in Social Media Expert response: Received multiple non-consecutive system messages.\n",
      "\n",
      "âŒ Error during conversation: Received multiple non-consecutive system messages.\n",
      "âŒ An error occurred during the conversation\n",
      "Conversation completed!\n"
     ]
    }
   ],
   "source": [
    "def chain_conversation(\n",
    "    query: str,\n",
    "    language: str,\n",
    "    app_openai,\n",
    "    app_anthropic,\n",
    "    num_turns: int = 2,\n",
    "    max_tokens: int = 1000\n",
    ") -> dict:\n",
    "    \n",
    "    conversation_history = []\n",
    "    metrics = {\"token_usage\": 0, \"cost\": 0}\n",
    "    \n",
    "    def process_model_response(\n",
    "        app, \n",
    "        message_content: str, \n",
    "        expert_name: str\n",
    "    ) -> str:\n",
    "        print(f\"\\nProcessing {expert_name} response...\")\n",
    "        \n",
    "        # Clean input message\n",
    "        clean_content = clean_message(message_content)\n",
    "        message = HumanMessage(content=clean_content)\n",
    "        \n",
    "        try:\n",
    "            # Use invoke instead of ainvoke\n",
    "            output = app.invoke(\n",
    "                {\"messages\": [message], \"language\": language},\n",
    "                config\n",
    "            )\n",
    "            \n",
    "            if not output or \"messages\" not in output or not output[\"messages\"]:\n",
    "                raise ValueError(\"No valid response received from model\")\n",
    "            \n",
    "            response = clean_message(output[\"messages\"][-1].content)\n",
    "            print(f\"{expert_name} responded: {response[:100]}...\")\n",
    "            \n",
    "            conversation_history.append({\n",
    "                \"role\": expert_name,\n",
    "                \"content\": response,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {expert_name} response: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        current_query = clean_message(query)\n",
    "        \n",
    "        for turn in range(num_turns):\n",
    "            print(f\"\\n=== Turn {turn + 1}/{num_turns} ===\")\n",
    "            \n",
    "            # Legal Expert Analysis\n",
    "            legal_response = process_model_response(\n",
    "                app_openai,\n",
    "                current_query,\n",
    "                \"Legal Expert\"\n",
    "            )\n",
    "            \n",
    "            # Social Media Expert Translation\n",
    "            social_prompt = clean_message(\n",
    "                f\"You are a social media expert. Please transform this legal explanation into engaging social media content \"\n",
    "                f\"while maintaining accuracy. Use {language} language. Legal explanation: {legal_response}\"\n",
    "            )\n",
    "            \n",
    "            social_response = process_model_response(\n",
    "                app_anthropic,\n",
    "                social_prompt,\n",
    "                \"Social Media Expert\"\n",
    "            )\n",
    "            \n",
    "            # Update query for next turn if needed\n",
    "            if turn < num_turns - 1:\n",
    "                current_query = clean_message(\n",
    "                    f\"Based on these explanations, what additional aspects of the EU AI Act should we explore? \"\n",
    "                    f\"Previous legal response: {legal_response} \"\n",
    "                    f\"Previous social media response: {social_response}\"\n",
    "                )\n",
    "        \n",
    "        # Generate final summary\n",
    "        summary_prompt = clean_message(\n",
    "            f\"Please provide a brief summary in {language} of the key points discussed about the EU AI Act in this conversation.\"\n",
    "        )\n",
    "        \n",
    "        summary = process_model_response(\n",
    "            app_anthropic,\n",
    "            summary_prompt,\n",
    "            \"Summary\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"original_query\": query,\n",
    "            \"conversation\": conversation_history,\n",
    "            \"summary\": summary,\n",
    "            \"metrics\": metrics,\n",
    "            \"language\": language\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error during conversation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def run_conversation():\n",
    "    query = \"Â¿QuÃ© es la EU AI Act y cÃ³mo afectarÃ¡ a las empresas?\"\n",
    "    language = \"Spanish\"\n",
    "    global config\n",
    "    config = {\"configurable\": {\"thread_id\": \"test123\"}}\n",
    "    \n",
    "    print(f\"\\nðŸ“ Query: {query}\")\n",
    "    print(\"\\nðŸ”„ Processing conversation...\\n\")\n",
    "    \n",
    "    result = chain_conversation(\n",
    "        query=query,\n",
    "        language=language,\n",
    "        app_openai=app_openai,\n",
    "        app_anthropic=app_anthropic,\n",
    "        num_turns=3\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\n=== Conversation Summary ===\\n\")\n",
    "        print(f\"ðŸŽ¯ Original Query: {result['original_query']}\\n\")\n",
    "        \n",
    "        for entry in result['conversation']:\n",
    "            print(f\"\\nðŸ‘¤ {entry['role']}:\")\n",
    "            print(f\"{entry['content']}\\n\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        print(\"\\nðŸ“‹ Final Summary:\")\n",
    "        print(f\"{result['summary']}\\n\")\n",
    "    else:\n",
    "        print(\"âŒ An error occurred during the conversation\")\n",
    "\n",
    "# Execute\n",
    "print(\"Starting the conversation...\")\n",
    "run_conversation()\n",
    "print(\"Conversation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def enhanced_chain_conversation(\n",
    "    query: str,\n",
    "    language: str,\n",
    "    num_turns: int = 2,\n",
    "    max_tokens: int = 1000\n",
    ") -> dict:\n",
    "    \n",
    "    conversation_history = []\n",
    "    metrics = {\"token_usage\": 0, \"cost\": 0}\n",
    "    \n",
    "    async def process_model_response(\n",
    "        app, \n",
    "        message_content: str, \n",
    "        expert_name: str\n",
    "    ) -> str:\n",
    "        # Clean input message\n",
    "        clean_content = clean_message(message_content)\n",
    "        \n",
    "        # Create message with cleaned content\n",
    "        message = HumanMessage(content=clean_content)\n",
    "        \n",
    "        # Get response from model\n",
    "        output = await app.ainvoke(\n",
    "            {\"messages\": [message], \"language\": language},\n",
    "            config\n",
    "        )\n",
    "        \n",
    "        if not output or \"messages\" not in output or not output[\"messages\"]:\n",
    "            raise ValueError(\"No valid response received from model\")\n",
    "        \n",
    "        # Clean the response content\n",
    "        response = clean_message(output[\"messages\"][-1].content)\n",
    "        \n",
    "        # Store in conversation history\n",
    "        conversation_history.append({\n",
    "            \"role\": expert_name,\n",
    "            \"content\": response,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    try:\n",
    "        # Clean initial query\n",
    "        current_query = clean_message(query)\n",
    "        \n",
    "        for turn in range(num_turns):\n",
    "            # Legal Expert Analysis\n",
    "            legal_response = await process_model_response(\n",
    "                app_openai,\n",
    "                current_query,\n",
    "                \"Legal Expert\"\n",
    "            )\n",
    "            \n",
    "            # Social Media Expert Translation\n",
    "            social_prompt = clean_message(\n",
    "                f\"Transform this legal explanation into engaging social media content while maintaining accuracy: {legal_response}\"\n",
    "            )\n",
    "            \n",
    "            social_response = await process_model_response(\n",
    "                app_anthropic,\n",
    "                social_prompt,\n",
    "                \"Social Media Expert\"\n",
    "            )\n",
    "            \n",
    "            # Update query for next turn if needed\n",
    "            if turn < num_turns - 1:\n",
    "                current_query = clean_message(\n",
    "                    f\"Based on these explanations, what additional insights would be helpful? Legal: {legal_response} Social: {social_response}\"\n",
    "                )\n",
    "        \n",
    "        # Generate final summary\n",
    "        summary_prompt = clean_message(\n",
    "            \"Provide a brief summary of the key points discussed in this conversation.\"\n",
    "        )\n",
    "        \n",
    "        summary = await process_model_response(\n",
    "            app_anthropic,\n",
    "            summary_prompt,\n",
    "            \"Summary\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"original_query\": query,\n",
    "            \"conversation\": conversation_history,\n",
    "            \"summary\": summary,\n",
    "            \"metrics\": metrics,\n",
    "            \"language\": language\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during conversation: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Sequence, Tuple, List\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "def clean_message(text: str) -> str:\n",
    "    \"\"\"Clean message content by removing excessive whitespace\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "async def enhanced_chain_conversation(\n",
    "    query: str,\n",
    "    language: str,\n",
    "    app_openai, \n",
    "    app_anthropic,\n",
    "    config: dict,\n",
    "    num_turns: int = 2,\n",
    "    max_tokens: int = 1000,\n",
    "   ) -> dict:\n",
    "    \n",
    "    conversation_history = []\n",
    "    metrics = {\"token_usage\": 0, \"cost\": 0}\n",
    "    \n",
    "    async def process_model_response(\n",
    "        app, \n",
    "        message_content: str, \n",
    "        expert_name: str\n",
    "    ) -> str:\n",
    "        # Create a clean message\n",
    "        clean_content = clean_message(message_content)\n",
    "        messages = {\"messages\": [HumanMessage(content=clean_content)], \"language\": language}\n",
    "        \n",
    "        # Get response from model\n",
    "        output = await app.ainvoke(messages, config)\n",
    "        \n",
    "        if not output or \"messages\" not in output or not output[\"messages\"]:\n",
    "            raise ValueError(\"No valid response received from model\")\n",
    "            \n",
    "        response = clean_message(output[\"messages\"][-1].content)\n",
    "        \n",
    "        # Update metrics (if output contains token usage/cost information)\n",
    "        if \"token_usage\" in output:\n",
    "            metrics[\"token_usage\"] += output[\"token_usage\"]\n",
    "        if \"cost\" in output:\n",
    "            metrics[\"cost\"] += output[\"cost\"]\n",
    "        \n",
    "        # Store in conversation history\n",
    "        conversation_history.append({\n",
    "            \"role\": expert_name,\n",
    "            \"content\": response,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    try:\n",
    "        # Initial query\n",
    "        current_query = query\n",
    "        \n",
    "        for turn in range(num_turns):\n",
    "            # Legal Expert Analysis\n",
    "            legal_response = await process_model_response(\n",
    "                app_openai,\n",
    "                current_query,\n",
    "                \"Legal Expert\"\n",
    "            )\n",
    "            \n",
    "            # Social Media Expert Translation\n",
    "            social_prompt = clean_message(\n",
    "                f\"Transform this legal explanation into engaging, clear content \"\n",
    "                f\"that would work well on social media while maintaining accuracy: {legal_response}\"\n",
    "            )\n",
    "            \n",
    "            social_response = await process_model_response(\n",
    "                app_anthropic,\n",
    "                social_prompt,\n",
    "                \"Social Media Expert\"\n",
    "            )\n",
    "            \n",
    "            # Update query for next turn if needed\n",
    "            if turn < num_turns - 1:\n",
    "                current_query = clean_message(\n",
    "                    \"Based on both explanations above, what additional insights \"\n",
    "                    f\"or clarifications would be helpful? Legal response: {legal_response} \"\n",
    "                    f\"Social media response: {social_response}\"\n",
    "                )\n",
    "        \n",
    "        # Generate final summary\n",
    "        summary_prompt = clean_message(\n",
    "            \"Please provide a brief, clear summary of the key points discussed \"\n",
    "            \"in our conversation about this topic.\"\n",
    "        )\n",
    "        \n",
    "        summary = await process_model_response(\n",
    "            app_anthropic,\n",
    "            summary_prompt,\n",
    "            \"Summary\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"original_query\": query,\n",
    "            \"conversation\": conversation_history,\n",
    "            \"summary\": summary,\n",
    "            \"metrics\": metrics,\n",
    "            \"language\": language\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during conversation: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_conversation():\n",
    "    query = \"Â¿QuÃ© es la EU AI Act y cÃ³mo afectarÃ¡ a las empresas?\"\n",
    "    language = \"Spanish\"\n",
    "    global config\n",
    "    config = {\"configurable\": {\"thread_id\": \"test123\"}}\n",
    "    \n",
    "    print(f\"\\nðŸ“ Query: {query}\")\n",
    "    print(\"\\nðŸ”„ Processing conversation...\\n\")\n",
    "    \n",
    "    result = await enhanced_chain_conversation(\n",
    "        query=query,\n",
    "        language=language,\n",
    "        app_openai=app_openai,  # Pass the OpenAI app\n",
    "        app_anthropic=app_anthropic  # Pass the Anthropic app\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\n=== Conversation Summary ===\\n\")\n",
    "        print(f\"ðŸŽ¯ Original Query: {result['original_query']}\\n\")\n",
    "        \n",
    "        for entry in result['conversation']:\n",
    "            print(f\"\\nðŸ‘¤ {entry['role']}:\")\n",
    "            print(f\"{entry['content']}\\n\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        print(\"\\nðŸ“‹ Final Summary:\")\n",
    "        print(f\"{result['summary']}\\n\")\n",
    "    else:\n",
    "        print(\"âŒ An error occurred during the conversation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Query: Â¿QuÃ© es la EU AI Act y cÃ³mo afectarÃ¡ a las empresas?\n",
      "\n",
      "ðŸ”„ Processing conversation...\n",
      "\n",
      "Error during conversation: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'messages: at least one message is required'}}\n",
      "âŒ An error occurred during the conversation\n"
     ]
    }
   ],
   "source": [
    "# Example usage with error handling and pretty printing\n",
    "async def run_conversation():\n",
    "    query = \"Â¿QuÃ© es la EU AI Act y cÃ³mo afectarÃ¡ a las empresas?\"\n",
    "    language = \"Spanish\"\n",
    "    global config  # Make sure config is accessible\n",
    "    config = {\"configurable\": {\"thread_id\": \"test123\"}}\n",
    "    \n",
    "    print(f\"\\nðŸ“ Query: {query}\")\n",
    "    print(\"\\nðŸ”„ Processing conversation...\\n\")\n",
    "    \n",
    "    result = await enhanced_chain_conversation(query, language)\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\n=== Conversation Summary ===\\n\")\n",
    "        print(f\"ðŸŽ¯ Original Query: {result['original_query']}\\n\")\n",
    "        \n",
    "        for entry in result['conversation']:\n",
    "            print(f\"\\nðŸ‘¤ {entry['role']}:\")\n",
    "            print(f\"{entry['content']}\\n\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        print(\"\\nðŸ“‹ Final Summary:\")\n",
    "        print(f\"{result['summary']}\\n\")\n",
    "    else:\n",
    "        print(\"âŒ An error occurred during the conversation\")\n",
    "\n",
    "# Run the conversation\n",
    "await run_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the conversation...\n",
      "\n",
      "ðŸ“ Query: Â¿QuÃ© es la EU AI Act y cÃ³mo afectarÃ¡ a las empresas?\n",
      "\n",
      "ðŸ”„ Processing conversation...\n",
      "\n",
      "\n",
      "Processing Legal Expert response...\n",
      "Legal Expert responded: La EU AI Act, que se refiere a la propuesta de Ley de Inteligencia Artificial de la UniÃ³n Europea, e...\n",
      "\n",
      "Processing Social Media Expert response...\n",
      "Social Media Expert responded: Â¡Hola! Hoy vamos a hablar de un tema importante y emocionante: Â¡La Ley de Inteligencia Artificial de...\n",
      "\n",
      "Processing Summary response...\n",
      "Error in Summary response: Received multiple non-consecutive system messages.\n",
      "\n",
      "âŒ Error during conversation: Received multiple non-consecutive system messages.\n",
      "âŒ An error occurred during the conversation\n",
      "Conversation completed!\n"
     ]
    }
   ],
   "source": [
    "async def enhanced_chain_conversation(\n",
    "    query: str,\n",
    "    language: str,\n",
    "    app_openai,\n",
    "    app_anthropic,\n",
    "    num_turns: int = 2,\n",
    "    max_tokens: int = 1000\n",
    ") -> dict:\n",
    "    \n",
    "    conversation_history = []\n",
    "    metrics = {\"token_usage\": 0, \"cost\": 0}\n",
    "    \n",
    "    async def process_model_response(\n",
    "        app, \n",
    "        message_content: str, \n",
    "        expert_name: str\n",
    "    ) -> str:\n",
    "        print(f\"\\nProcessing {expert_name} response...\")\n",
    "        \n",
    "        # Create a clean message\n",
    "        clean_content = clean_message(message_content)\n",
    "        \n",
    "        # Different message structure for each model\n",
    "        if expert_name == \"Legal Expert\":\n",
    "            messages = {\"messages\": [HumanMessage(content=clean_content)], \"language\": language}\n",
    "        else:\n",
    "            # For Anthropic, include a system message\n",
    "            messages = {\n",
    "                \"messages\": [\n",
    "                    SystemMessage(content=\"You are a social media expert. Convert complex information into engaging, clear content.\"),\n",
    "                    HumanMessage(content=clean_content)\n",
    "                ],\n",
    "                \"language\": language\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            output = await app.ainvoke(messages, config)\n",
    "            \n",
    "            if not output or \"messages\" not in output or not output[\"messages\"]:\n",
    "                raise ValueError(\"No valid response received from model\")\n",
    "            \n",
    "            response = clean_message(output[\"messages\"][-1].content)\n",
    "            print(f\"{expert_name} responded: {response[:100]}...\")\n",
    "            \n",
    "            conversation_history.append({\n",
    "                \"role\": expert_name,\n",
    "                \"content\": response,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {expert_name} response: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        current_query = clean_message(query)\n",
    "        \n",
    "        # Legal Expert Analysis\n",
    "        legal_response = await process_model_response(\n",
    "            app_openai,\n",
    "            current_query,\n",
    "            \"Legal Expert\"\n",
    "        )\n",
    "        \n",
    "        # Social Media Expert Translation\n",
    "        social_prompt = (\n",
    "            \"Transform this legal explanation into engaging social media content. \"\n",
    "            \"Keep it clear and accurate, but make it more accessible and interesting \"\n",
    "            f\"for a general audience: {legal_response}\"\n",
    "        )\n",
    "        \n",
    "        social_response = await process_model_response(\n",
    "            app_anthropic,\n",
    "            social_prompt,\n",
    "            \"Social Media Expert\"\n",
    "        )\n",
    "        \n",
    "        # Generate final summary\n",
    "        summary_prompt = (\n",
    "            \"Please provide a brief, clear summary of the key points discussed \"\n",
    "            \"in our conversation about the EU AI Act.\"\n",
    "        )\n",
    "        \n",
    "        summary = await process_model_response(\n",
    "            app_anthropic,\n",
    "            summary_prompt,\n",
    "            \"Summary\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"original_query\": query,\n",
    "            \"conversation\": conversation_history,\n",
    "            \"summary\": summary,\n",
    "            \"metrics\": metrics,\n",
    "            \"language\": language\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error during conversation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run the conversation\n",
    "async def run_conversation():\n",
    "    query = \"Â¿QuÃ© es la EU AI Act y cÃ³mo afectarÃ¡ a las empresas?\"\n",
    "    language = \"Spanish\"\n",
    "    global config\n",
    "    config = {\"configurable\": {\"thread_id\": \"test123\"}}\n",
    "    \n",
    "    print(f\"\\nðŸ“ Query: {query}\")\n",
    "    print(\"\\nðŸ”„ Processing conversation...\\n\")\n",
    "    \n",
    "    result = await enhanced_chain_conversation(\n",
    "        query=query,\n",
    "        language=language,\n",
    "        app_openai=app_openai,\n",
    "        app_anthropic=app_anthropic\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\n=== Conversation Summary ===\\n\")\n",
    "        print(f\"ðŸŽ¯ Original Query: {result['original_query']}\\n\")\n",
    "        \n",
    "        for entry in result['conversation']:\n",
    "            print(f\"\\nðŸ‘¤ {entry['role']}:\")\n",
    "            print(f\"{entry['content']}\\n\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        print(\"\\nðŸ“‹ Final Summary:\")\n",
    "        print(f\"{result['summary']}\\n\")\n",
    "    else:\n",
    "        print(\"âŒ An error occurred during the conversation\")\n",
    "\n",
    "# Execute\n",
    "print(\"Starting the conversation...\")\n",
    "await run_conversation()\n",
    "print(\"Conversation completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-wise-council-U8tERo2L-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
