{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "\n",
    "from typing import Sequence, Callable, Any\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_openai = ChatOpenAI(model=\"gpt-4o-mini\", max_completion_tokens=100)\n",
    "model_anthropic = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_openai, model_anthropic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    prompt_template: ChatPromptTemplate\n",
    "    model_used: BaseChatModel\n",
    "    language: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "import langchain\n",
    "\n",
    "langchain.cache = InMemoryCache()\n",
    "# Or for persistent caching:\n",
    "# from langchain.cache import SQLiteCache\n",
    "# langchain.cache = SQLiteCache(database_path=\".langchain.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimmer(model: BaseChatModel) -> Callable[[MessagesState], list[BaseMessage]]:\n",
    "    return trim_messages(\n",
    "        max_tokens=50,\n",
    "        strategy=\"last\",\n",
    "        token_counter=model,\n",
    "        include_system=True,\n",
    "        allow_partial=True,\n",
    "        start_on=\"human\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_first_layer_model(state: State) -> dict[str, list[BaseMessage]]:\n",
    "    trimmed_messages = trimmer(model=state[\"model_used\"]).invoke(state[\"messages\"])\n",
    "    template = state[\"prompt_template\"]\n",
    "    model = state[\"model_used\"]\n",
    "    prompt = template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state.get(\"language\", \"English\")}, config)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lawyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_lawyer = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You are an experienced attorney with decades of expertise, known for your thorough understanding of legal frameworks and your ability to articulate precise, formal, and well-reasoned arguments. \"\n",
    "                \"Your goal is to provide comprehensive, professional responses that reflect a deep knowledge of the law, ethical considerations, and relevant legal principles. \"\n",
    "                \"When appropriate, reference specific statutes, regulations, case law, or precedents to support your response. For example, in U.S. law, you might cite Marbury v. Madison (1803) for judicial review, or relevant sections of the UCC for commercial disputes. \"\n",
    "                \"Structure your response logically with clear headings and subheadings if the question involves multiple aspects. Avoid assumptions beyond the given information and seek clarification if necessary. \"\n",
    "                \"Always communicate in {language}, maintaining a professional and respectful tone, ensuring accessibility for both legal experts and non-specialists. \"\n",
    "                \"Conclude your response with a summary or actionable next steps if applicable.\"\n",
    "            ),\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(\n",
    "    schema: dict[str, Any], \n",
    "    function: Callable[[State], dict[str, list[BaseMessage]]]\n",
    "):\n",
    "    workflow = StateGraph(state_schema=schema)\n",
    "    workflow.add_edge(START, \"llm_call\")\n",
    "    workflow.add_node(\"llm_call\", function)\n",
    "    return workflow.compile(checkpointer=MemorySaver())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola Ana,\n",
      "\n",
      "### Introducción a la EU AI Act\n",
      "\n",
      "La **EU AI Act** (Reglamento sobre Inteligencia Artificial de la Unión Europea) es una propuesta de legislación presentada por la Comisión Europea con el objetivo de regular el desarrollo y uso de tecnologías de inteligencia artificial (IA) en la Unión Europea. Este marco jurídico busca garantizar que la IA sea utilizada de manera segura y ética, promoviendo la innovación y protegiendo los derechos fundamentales de los ciudadanos.\n",
      "\n",
      "### Objetivos Principales\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Hola, me llamo Ana. Me gustaria saber que es la EU AI Act.\"\n",
    "language = \"Spanish\"\n",
    "config = {\"configurable\": {\"thread_id\": \"aaa123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output_lawyer = create_graph(State, call_first_layer_model).invoke({\n",
    "    \"messages\": input_messages, \n",
    "    \"language\": language,\n",
    "    \"prompt_template\": prompt_template_lawyer,\n",
    "    \"model_used\": model_openai,\n",
    "    }, \n",
    "config)\n",
    "print(output_lawyer[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_social = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You are a seasoned social media strategist with extensive experience in creating highly engaging and platform-specific content across Instagram, TikTok, YouTube, LinkedIn, and other major platforms. \"\n",
    "                \"Your expertise includes crafting strategies that drive audience growth, boost engagement, and optimize monetization opportunities. \"\n",
    "                \"Provide practical, actionable advice tailored to the platform in focus. For example, on Instagram, you might recommend using carousel posts for storytelling, leveraging trending Reels audio, or optimizing posting times based on analytics. \"\n",
    "                \"On TikTok, suggest ideas for viral trends, short-form video hooks, or creative hashtag usage. \"\n",
    "                \"Your recommendations should reflect the latest features and trends, such as LinkedIn’s algorithm preferences for professional storytelling or YouTube Shorts for reaching broader audiences. \"\n",
    "                \"When discussing analytics, explain how to track and interpret key metrics like engagement rates, click-through rates (CTR), or watch time, providing examples where relevant. \"\n",
    "                \"Offer community-building strategies, such as fostering meaningful interactions in comment sections or collaborating with influencers, to strengthen brand loyalty. \"\n",
    "                \"Communicate in {language}, maintaining an engaging and approachable tone suitable for both novice and experienced content creators. \"\n",
    "                \"Ensure your response includes real-world examples, practical tips, and clear steps for implementation.\"\n",
    "            ),\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola, Ana! Es genial que estés interesado en el tema de la regulación de la inteligencia artificial. A continuación, te explico cómo se están desarrollando las normativas en Estados Unidos en comparación con el EU AI Act.\n",
      "\n",
      "### Equivalente del EU AI Act en EE. UU.\n",
      "\n",
      "1. **No hay una ley federal única**: A diferencia de la Unión Europea, donde se está trabajando en un marco único (el EU AI Act) para regular la inteligencia artificial, en Estados Unidos\n"
     ]
    }
   ],
   "source": [
    "query = \"Me gustaria saber el equivalente de la EU AI Act en USA\"\n",
    "language = \"Spanish\"\n",
    "config = {\"configurable\": {\"thread_id\": \"aaa123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output_social = create_graph(State, call_first_layer_model).invoke({\n",
    "    \"messages\": input_messages, \n",
    "    \"language\": language,\n",
    "    \"prompt_template\": prompt_template_social,\n",
    "    \"model_used\": model_openai,\n",
    "    }, \n",
    "config)\n",
    "print(output_social[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizerState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    prompt_template: ChatPromptTemplate\n",
    "    model_used: BaseChatModel\n",
    "    language: str\n",
    "    output_experts: list[dict[str, list[BaseMessage]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_second_layer_model(state: SummarizerState) -> dict[str, list[BaseMessage]]:\n",
    "    trimmed_messages = trimmer(model=state[\"model_used\"]).invoke(state[\"messages\"])\n",
    "    template = state[\"prompt_template\"]\n",
    "    model = state[\"model_used\"]\n",
    "    output_experts = \"\".join([f\" Expert {i+1}: {expert['messages'][-1].content}\" for i, expert in enumerate(state[\"output_experts\"])])\n",
    "\n",
    "    prompt = template.invoke({\n",
    "        \"messages\": trimmed_messages, \n",
    "        \"language\": state.get(\"language\", \"English\"),\n",
    "        \"output_experts\": output_experts\n",
    "    }, config)\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    output_summarizer = f\"\"\"The experts have provided the following information: {output_experts}\n",
    "        The summary of the information is: {response}\n",
    "    \"\"\"\n",
    "    return {\"messages\": [output_summarizer]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_summarizer = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\"\"\"\n",
    "                You are an expert summarizer with exceptional skills in synthesizing and distilling information from diverse sources.\n",
    "                Your task is to combine the key insights from the experts provided here: {output_experts}.\n",
    "                Craft a clear, concise, and cohesive summary that integrates their perspectives effectively, respecting each expert's unique focus and intent.\\n\\n\n",
    "                Specifications:\n",
    "                   - Provide a concise synthesis that integrates all perspectives.\\n\n",
    "                   - Ensure the summary is engaging, easy to understand, and tailored to the intended audience.\\n\\n\n",
    "                When summarizing:\\n\n",
    "                - Maintain a professional yet conversational tone, ensuring accessibility for readers with varying levels of expertise.\\n\n",
    "                - Use analogies, examples, or simplified explanations to enhance understanding when complex concepts are involved.\\n\n",
    "                - Format your response with bullet points, headings, or short paragraphs for readability.\\n\\n\n",
    "                Your response must be in {language} and must align with the style and context appropriate to the provided audience.\n",
    "            \"\"\"),\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        The experts have provided the following information:  Expert 1: Hola Ana,\n",
      "\n",
      "### Introducción a la EU AI Act\n",
      "\n",
      "La **EU AI Act** (Reglamento sobre Inteligencia Artificial de la Unión Europea) es una propuesta de legislación presentada por la Comisión Europea con el objetivo de regular el desarrollo y uso de tecnologías de inteligencia artificial (IA) en la Unión Europea. Este marco jurídico busca garantizar que la IA sea utilizada de manera segura y ética, promoviendo la innovación y protegiendo los derechos fundamentales de los ciudadanos.\n",
      "\n",
      "### Objetivos Principales\n",
      "\n",
      " Expert 2: ¡Hola, Ana! Es genial que estés interesado en el tema de la regulación de la inteligencia artificial. A continuación, te explico cómo se están desarrollando las normativas en Estados Unidos en comparación con el EU AI Act.\n",
      "\n",
      "### Equivalente del EU AI Act en EE. UU.\n",
      "\n",
      "1. **No hay una ley federal única**: A diferencia de la Unión Europea, donde se está trabajando en un marco único (el EU AI Act) para regular la inteligencia artificial, en Estados Unidos\n",
      "        The summary of the information is: content='### Introducción a la Regulación de la IA en la UE y EE. UU.\\n\\nLa regulación de la inteligencia artificial (IA) es un tema clave en el desarrollo tecnológico actual, y distintos enfoques están siendo adoptados en la Unión Europea y Estados Unidos.\\n\\n#### EU AI Act\\n\\n- **Objetivo**: La **EU AI Act** es una propuesta legislativa presentada por la Comisión Europea diseñada para regular el uso y desarrollo de tecnologías de IA en la UE.\\n- **Enfoque' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 100, 'prompt_tokens': 407, 'total_tokens': 507, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'length', 'logprobs': None} id='run-4d1c3b07-57c5-4120-ab11-92b5fa2b9836-0' usage_metadata={'input_tokens': 407, 'output_tokens': 100, 'total_tokens': 507, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "query = \"Sintetiza los inputs de los expertos\"\n",
    "language = \"Spanish\"\n",
    "config = {\"configurable\": {\"thread_id\": \"aaa124\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output_summarizer = create_graph(SummarizerState, call_second_layer_model).invoke({\n",
    "    \"messages\": input_messages, \n",
    "    \"language\": language,\n",
    "    \"prompt_template\": prompt_template_summarizer,\n",
    "    \"model_used\": model_openai,\n",
    "    \"output_experts\":[output_lawyer, output_social]\n",
    "    }, \n",
    "config)\n",
    "print(output_summarizer[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aquí te presento una explicación del EU AI Act (Ley de Inteligencia Artificial de la Unión Europea) en español:\n",
      "\n",
      "El EU AI Act es una propuesta de regulación de la Unión Europea para abordar los riesgos y desafíos asociados con el desarrollo y uso de la inteligencia artificial (IA). Los principales puntos clave son:\n",
      "\n",
      "1. Ámbito de aplicación: La ley abarca todas las aplicaciones de IA, tanto públicas como privadas, que se desarrollen o utilicen en la UE.\n",
      "\n",
      "2. Clasificación de sistemas de IA: Se establece una clasificación de sistemas de IA en función del nivel de riesgo que conllevan:\n",
      "   - IA de riesgo inaceptable (prohibida)\n",
      "   - IA de alto riesgo (sujeta a requisitos obligatorios)\n",
      "   - IA de bajo o mínimo riesgo (sólo requiere transparencia)\n",
      "\n",
      "3. Requisitos para sistemas de IA de alto riesgo:\n",
      "   - Evaluación de riesgos y mitigación\n",
      "   - Documentación y trazabilidad\n",
      "   - Supervisión humana\n",
      "   - Robustez, precisión y seguridad\n",
      "\n",
      "4. Gobernanza y supervisión: Se crea un nuevo organismo europeo de supervisión de la IA.\n",
      "\n",
      "5. Sanciones: Se establecen multas de hasta el 6% del volumen de negocios anual por incumplimiento.\n",
      "\n",
      "En resumen, el EU AI Act busca establecer un marco regulatorio equilibrado que promueva la innovación tecnológica al tiempo que protege los derechos fundamentales y la seguridad de los ciudadanos europeos.\n"
     ]
    }
   ],
   "source": [
    "query = \"Hola, me llamo Ana. Me gustaria saber que es la EU AI Act.\"\n",
    "language = \"Spanish\"\n",
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = await app_anthropic.ainvoke({\"messages\": input_messages, \"language\": language}, config)\n",
    "print(output[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple, List\n",
    "from langchain_core.messages import AIMessage\n",
    "from datetime import datetime\n",
    "\n",
    "def clean_message(text: str) -> str:\n",
    "    \"\"\"Clean message content by removing ALL whitespace issues\"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return \"\"\n",
    "    # Remove ALL problematic whitespace\n",
    "    text = text.strip()  # Remove leading/trailing whitespace\n",
    "    text = ' '.join(text.split())  # Replace multiple spaces with single space\n",
    "    text = text.rstrip()  # Extra insurance against trailing whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola, me llamo Ana. Me gustaria saber que es la EU AI Act.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_message(\"     Hola, me llamo Ana. Me gustaria saber que es la EU AI Act.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def enhanced_chain_conversation(\n",
    "    query: str,\n",
    "    language: str,\n",
    "    app_openai,\n",
    "    app_anthropic,\n",
    "    num_turns: int = 2,\n",
    "    max_tokens: int = 10000\n",
    ") -> dict:\n",
    "    \n",
    "    conversation_history = []\n",
    "    metrics = {\"token_usage\": 0, \"cost\": 0}\n",
    "    \n",
    "    async def process_model_response(\n",
    "        app, \n",
    "        message_content: str, \n",
    "        expert_name: str\n",
    "    ) -> str:\n",
    "        print(f\"\\nProcessing {expert_name} response...\")\n",
    "        \n",
    "        # Create a clean message\n",
    "        clean_content = clean_message(message_content)\n",
    "        \n",
    "        # Different message structure for each model, now with max_tokens\n",
    "        if expert_name == \"Legal Expert\":\n",
    "            messages = {\n",
    "                \"messages\": [HumanMessage(content=clean_content)], \n",
    "                \"language\": language,\n",
    "                \"max_tokens\": max_tokens  # Use the max_tokens parameter\n",
    "            }\n",
    "        else:\n",
    "            messages = {\n",
    "                \"messages\": [\n",
    "                    SystemMessage(content=\"You are a social media expert. Convert complex information into engaging, clear content.\"),\n",
    "                    HumanMessage(content=clean_content)\n",
    "                ],\n",
    "                \"language\": language,\n",
    "                \"max_tokens\": max_tokens  # Use the max_tokens parameter\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            output = await app.ainvoke(messages, config)\n",
    "            response = clean_message(output[\"messages\"][-1].content)\n",
    "            print(f\"{expert_name} responded: {response[:100]}...\")\n",
    "            \n",
    "            conversation_history.append({\n",
    "                \"role\": expert_name,\n",
    "                \"content\": response,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {expert_name} response: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        current_query = clean_message(query)\n",
    "        \n",
    "        # Now use num_turns for multiple exchanges\n",
    "        for turn in range(num_turns):\n",
    "            print(f\"\\n=== Turn {turn + 1}/{num_turns} ===\")\n",
    "            \n",
    "            # Legal Expert Analysis\n",
    "            legal_response = process_model_response(\n",
    "                app_openai,\n",
    "                current_query,\n",
    "                \"Legal Expert\"\n",
    "            )\n",
    "            \n",
    "            # Social Media Expert Translation\n",
    "            social_prompt = (\n",
    "                \"Transform this legal explanation into engaging social media content. \"\n",
    "                \"Keep it clear and accurate, but make it more accessible and interesting \"\n",
    "                f\"for a general audience: {legal_response}\"\n",
    "            )\n",
    "            \n",
    "            social_response = process_model_response(\n",
    "                app_anthropic,\n",
    "                social_prompt,\n",
    "                \"Social Media Expert\"\n",
    "            )\n",
    "            \n",
    "            # Update query for next turn if there are more turns\n",
    "            if turn < num_turns - 1:\n",
    "                current_query = (\n",
    "                    \"Based on the previous responses, please provide additional insights \"\n",
    "                    \"or explore another aspect of the EU AI Act. \"\n",
    "                    f\"Previous legal response: {legal_response} \"\n",
    "                    f\"Previous social media response: {social_response}\"\n",
    "                )\n",
    "        \n",
    "        # Generate final summary\n",
    "        summary_prompt = (\n",
    "            \"Please provide a brief, clear summary of all the key points discussed \"\n",
    "            \"in our conversation about the EU AI Act.\"\n",
    "        )\n",
    "        \n",
    "        summary = process_model_response(\n",
    "            app_anthropic,\n",
    "            summary_prompt,\n",
    "            \"Summary\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"original_query\": query,\n",
    "            \"conversation\": conversation_history,\n",
    "            \"summary\": summary,\n",
    "            \"metrics\": metrics,\n",
    "            \"language\": language\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error during conversation: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<coroutine object enhanced_chain_conversation at 0x000002064BE7F120>\n"
     ]
    }
   ],
   "source": [
    "# Example usage with specific turns and token limit\n",
    "result = enhanced_chain_conversation(\n",
    "    query=query,\n",
    "    language=language,\n",
    "    app_openai=app_openai,\n",
    "    app_anthropic=app_anthropic,\n",
    "    num_turns=3,  # Will do 3 rounds of exchanges\n",
    "    #max_tokens=800  # Limit response lengths\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chain_conversation(\n",
    "    query: str,\n",
    "    language: str,\n",
    "    app_openai,\n",
    "    app_anthropic,\n",
    "    num_turns: int,\n",
    ") -> dict:\n",
    "    \n",
    "    conversation_history = []\n",
    "        \n",
    "    async def process_model_response(\n",
    "        app, \n",
    "        message_content: str, \n",
    "        expert_name: str,\n",
    "    ) -> str:\n",
    "        print(f\"\\nProcessing {expert_name} response...\")\n",
    "        \n",
    "        clean_content = clean_message(message_content)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # Different message structure for each model\n",
    "        if expert_name == \"Legal Expert\":\n",
    "            messages = {\n",
    "                \"messages\": [HumanMessage(content=clean_content)],\n",
    "                \"language\": language\n",
    "            }\n",
    "        else:\n",
    "            # For Anthropic, only include system message when specified\n",
    "            message_list = []\n",
    "            if include_system:\n",
    "                message_list.append(\n",
    "                    SystemMessage(content=\"You are a social media expert. Convert complex information into engaging, clear content.\")\n",
    "                )\n",
    "            message_list.append(HumanMessage(content=clean_content))\n",
    "            messages = {\n",
    "                \"messages\": message_list,\n",
    "                \"language\": language\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            output = await app.ainvoke(messages, config)\n",
    "            response = clean_message(output[\"messages\"][-1].content)\n",
    "            print(f\"{expert_name} responded: {response[:100]}...\")\n",
    "            \n",
    "            conversation_history.append({\n",
    "                \"role\": expert_name,\n",
    "                \"content\": response,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {expert_name} response: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        current_query = clean_message(query)\n",
    "        \n",
    "        for turn in range(num_turns):\n",
    "            print(f\"\\n=== Turn {turn + 1}/{num_turns} ===\")\n",
    "            \n",
    "            # Legal Expert Analysis\n",
    "            legal_response = await process_model_response(\n",
    "                app_openai,\n",
    "                current_query,\n",
    "                \"Legal Expert\"\n",
    "            )\n",
    "            \n",
    "            # Social Media Expert Translation\n",
    "            # Only include system message in first turn\n",
    "            social_prompt = (\n",
    "                \"Transform this legal explanation into engaging social media content \"\n",
    "                \"while maintaining accuracy and using appropriate language. \"\n",
    "                f\"Legal explanation: {legal_response}\"\n",
    "            )\n",
    "            \n",
    "            social_response = await process_model_response(\n",
    "                app_anthropic,\n",
    "                social_prompt,\n",
    "                \"Social Media Expert\",\n",
    "                include_system=(turn == 0)  # Only include system message in first turn\n",
    "            )\n",
    "            \n",
    "            # Update query for next turn\n",
    "            if turn < num_turns - 1:\n",
    "                current_query = clean_message(\n",
    "                    \"Based on the previous responses, please provide additional insights \"\n",
    "                    \"about a different aspect of the EU AI Act that hasn't been covered yet. \"\n",
    "                    f\"Previous legal response: {legal_response}\"\n",
    "                )\n",
    "        \n",
    "        # Generate final summary without system message\n",
    "        summary_prompt = clean_message(\n",
    "            \"Please provide a brief, clear summary of all the key points discussed \"\n",
    "            \"about the EU AI Act in our conversation.\"\n",
    "        )\n",
    "        \n",
    "        summary = await process_model_response(\n",
    "            app_anthropic,\n",
    "            summary_prompt,\n",
    "            \"Summary\",\n",
    "            include_system=False  # Don't include system message for summary\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"original_query\": query,\n",
    "            \"conversation\": conversation_history,\n",
    "            \"summary\": summary,\n",
    "            \"metrics\": metrics,\n",
    "            \"language\": language\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error during conversation: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Turn 1/3 ===\n",
      "\n",
      "Processing Legal Expert response...\n",
      "Legal Expert responded: Hola, Ana. La EU AI Act, conocida formalmente como la Ley de Inteligencia Artificial de la Unión Eur...\n",
      "\n",
      "Processing Social Media Expert response...\n",
      "Error in Social Media Expert response: Received multiple non-consecutive system messages.\n",
      "\n",
      "❌ Error during conversation: Received multiple non-consecutive system messages.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "result = await enhanced_chain_conversation(\n",
    "    query=query,\n",
    "    language=language,\n",
    "    app_openai=app_openai,\n",
    "    app_anthropic=app_anthropic,\n",
    "    num_turns=3\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Query: ¿Qué es la EU AI Act y cómo afectará a las empresas?\n",
      "\n",
      "🔄 Processing conversation...\n",
      "\n",
      "\n",
      "=== Turn 1/3 ===\n",
      "\n",
      "Processing Legal Expert response...\n",
      "\n",
      "Processing Social Media Expert response...\n",
      "\n",
      "❌ Error during conversation: Received multiple non-consecutive system messages.\n",
      "❌ An error occurred during the conversation\n"
     ]
    }
   ],
   "source": [
    "async def enhanced_chain_conversation(\n",
    "    query: str,\n",
    "    language: str,\n",
    "    app_openai,\n",
    "    app_anthropic,\n",
    "    num_turns: int = 2,\n",
    "    max_tokens: int = 1000\n",
    ") -> dict:\n",
    "    \n",
    "    conversation_history = []\n",
    "    metrics = {\"token_usage\": 0, \"cost\": 0}\n",
    "    \n",
    "    async def process_model_response(\n",
    "        app, \n",
    "        message_content: str, \n",
    "        expert_name: str\n",
    "    ) -> str:\n",
    "        print(f\"\\nProcessing {expert_name} response...\")\n",
    "        \n",
    "        # Clean input message\n",
    "        clean_content = clean_message(message_content)\n",
    "        \n",
    "        # Create message with cleaned content\n",
    "        message = HumanMessage(content=clean_content)\n",
    "        \n",
    "        # Get response from model\n",
    "        output = await app.ainvoke(\n",
    "            {\"messages\": [message], \"language\": language},\n",
    "            config\n",
    "        )\n",
    "        \n",
    "        if not output or \"messages\" not in output or not output[\"messages\"]:\n",
    "            raise ValueError(\"No valid response received from model\")\n",
    "        \n",
    "        # Clean the response content\n",
    "        response = clean_message(output[\"messages\"][-1].content)\n",
    "        \n",
    "        # Store in conversation history\n",
    "        conversation_history.append({\n",
    "            \"role\": expert_name,\n",
    "            \"content\": response,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    try:\n",
    "        # Clean initial query\n",
    "        current_query = clean_message(query)\n",
    "        \n",
    "        for turn in range(num_turns):\n",
    "            print(f\"\\n=== Turn {turn + 1}/{num_turns} ===\")\n",
    "            \n",
    "            # Legal Expert Analysis\n",
    "            legal_response = await process_model_response(\n",
    "                app_openai,\n",
    "                current_query,\n",
    "                \"Legal Expert\"\n",
    "            )\n",
    "            \n",
    "            # Social Media Expert Translation\n",
    "            social_prompt = clean_message(\n",
    "                f\"You are a social media expert. Please transform this legal explanation into engaging social media content \"\n",
    "                f\"while maintaining accuracy. Use {language} language. Legal explanation: {legal_response}\"\n",
    "            )\n",
    "            \n",
    "            social_response = await process_model_response(\n",
    "                app_anthropic,\n",
    "                social_prompt,\n",
    "                \"Social Media Expert\"\n",
    "            )\n",
    "            \n",
    "            # Update query for next turn if needed\n",
    "            if turn < num_turns - 1:\n",
    "                current_query = clean_message(\n",
    "                    f\"Based on these explanations, what additional aspects of the EU AI Act should we explore? \"\n",
    "                    f\"Previous legal response: {legal_response} \"\n",
    "                    f\"Previous social media response: {social_response}\"\n",
    "                )\n",
    "        \n",
    "        # Generate final summary\n",
    "        summary_prompt = clean_message(\n",
    "            f\"Please provide a brief summary in {language} of the key points discussed about the EU AI Act in this conversation.\"\n",
    "        )\n",
    "        \n",
    "        summary = await process_model_response(\n",
    "            app_anthropic,\n",
    "            summary_prompt,\n",
    "            \"Summary\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"original_query\": query,\n",
    "            \"conversation\": conversation_history,\n",
    "            \"summary\": summary,\n",
    "            \"metrics\": metrics,\n",
    "            \"language\": language\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error during conversation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "async def run_conversation():\n",
    "    query = \"¿Qué es la EU AI Act y cómo afectará a las empresas?\"\n",
    "    language = \"Spanish\"\n",
    "    global config\n",
    "    config = {\"configurable\": {\"thread_id\": \"test123\"}}\n",
    "    \n",
    "    print(f\"\\n📝 Query: {query}\")\n",
    "    print(\"\\n🔄 Processing conversation...\\n\")\n",
    "    \n",
    "    result = await enhanced_chain_conversation(\n",
    "        query=query,\n",
    "        language=language,\n",
    "        app_openai=app_openai,\n",
    "        app_anthropic=app_anthropic,\n",
    "        num_turns=3\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\n=== Conversation Summary ===\\n\")\n",
    "        print(f\"🎯 Original Query: {result['original_query']}\\n\")\n",
    "        \n",
    "        for entry in result['conversation']:\n",
    "            print(f\"\\n👤 {entry['role']}:\")\n",
    "            print(f\"{entry['content']}\\n\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        print(\"\\n📋 Final Summary:\")\n",
    "        print(f\"{result['summary']}\\n\")\n",
    "    else:\n",
    "        print(\"❌ An error occurred during the conversation\")\n",
    "\n",
    "# Execute\n",
    "await run_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the conversation...\n",
      "\n",
      "📝 Query: ¿Qué es la EU AI Act y cómo afectará a las empresas?\n",
      "\n",
      "🔄 Processing conversation...\n",
      "\n",
      "\n",
      "=== Turn 1/3 ===\n",
      "\n",
      "Processing Legal Expert response...\n",
      "Legal Expert responded: La EU AI Act, formalmente conocida como el Reglamento sobre la Inteligencia Artificial de la Unión E...\n",
      "\n",
      "Processing Social Media Expert response...\n",
      "Error in Social Media Expert response: Received multiple non-consecutive system messages.\n",
      "\n",
      "❌ Error during conversation: Received multiple non-consecutive system messages.\n",
      "❌ An error occurred during the conversation\n",
      "Conversation completed!\n"
     ]
    }
   ],
   "source": [
    "def chain_conversation(\n",
    "    query: str,\n",
    "    language: str,\n",
    "    app_openai,\n",
    "    app_anthropic,\n",
    "    num_turns: int = 2,\n",
    "    max_tokens: int = 1000\n",
    ") -> dict:\n",
    "    \n",
    "    conversation_history = []\n",
    "    metrics = {\"token_usage\": 0, \"cost\": 0}\n",
    "    \n",
    "    def process_model_response(\n",
    "        app, \n",
    "        message_content: str, \n",
    "        expert_name: str\n",
    "    ) -> str:\n",
    "        print(f\"\\nProcessing {expert_name} response...\")\n",
    "        \n",
    "        # Clean input message\n",
    "        clean_content = clean_message(message_content)\n",
    "        message = HumanMessage(content=clean_content)\n",
    "        \n",
    "        try:\n",
    "            # Use invoke instead of ainvoke\n",
    "            output = app.invoke(\n",
    "                {\"messages\": [message], \"language\": language},\n",
    "                config\n",
    "            )\n",
    "            \n",
    "            if not output or \"messages\" not in output or not output[\"messages\"]:\n",
    "                raise ValueError(\"No valid response received from model\")\n",
    "            \n",
    "            response = clean_message(output[\"messages\"][-1].content)\n",
    "            print(f\"{expert_name} responded: {response[:100]}...\")\n",
    "            \n",
    "            conversation_history.append({\n",
    "                \"role\": expert_name,\n",
    "                \"content\": response,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {expert_name} response: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        current_query = clean_message(query)\n",
    "        \n",
    "        for turn in range(num_turns):\n",
    "            print(f\"\\n=== Turn {turn + 1}/{num_turns} ===\")\n",
    "            \n",
    "            # Legal Expert Analysis\n",
    "            legal_response = process_model_response(\n",
    "                app_openai,\n",
    "                current_query,\n",
    "                \"Legal Expert\"\n",
    "            )\n",
    "            \n",
    "            # Social Media Expert Translation\n",
    "            social_prompt = clean_message(\n",
    "                f\"You are a social media expert. Please transform this legal explanation into engaging social media content \"\n",
    "                f\"while maintaining accuracy. Use {language} language. Legal explanation: {legal_response}\"\n",
    "            )\n",
    "            \n",
    "            social_response = process_model_response(\n",
    "                app_anthropic,\n",
    "                social_prompt,\n",
    "                \"Social Media Expert\"\n",
    "            )\n",
    "            \n",
    "            # Update query for next turn if needed\n",
    "            if turn < num_turns - 1:\n",
    "                current_query = clean_message(\n",
    "                    f\"Based on these explanations, what additional aspects of the EU AI Act should we explore? \"\n",
    "                    f\"Previous legal response: {legal_response} \"\n",
    "                    f\"Previous social media response: {social_response}\"\n",
    "                )\n",
    "        \n",
    "        # Generate final summary\n",
    "        summary_prompt = clean_message(\n",
    "            f\"Please provide a brief summary in {language} of the key points discussed about the EU AI Act in this conversation.\"\n",
    "        )\n",
    "        \n",
    "        summary = process_model_response(\n",
    "            app_anthropic,\n",
    "            summary_prompt,\n",
    "            \"Summary\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"original_query\": query,\n",
    "            \"conversation\": conversation_history,\n",
    "            \"summary\": summary,\n",
    "            \"metrics\": metrics,\n",
    "            \"language\": language\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error during conversation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def run_conversation():\n",
    "    query = \"¿Qué es la EU AI Act y cómo afectará a las empresas?\"\n",
    "    language = \"Spanish\"\n",
    "    global config\n",
    "    config = {\"configurable\": {\"thread_id\": \"test123\"}}\n",
    "    \n",
    "    print(f\"\\n📝 Query: {query}\")\n",
    "    print(\"\\n🔄 Processing conversation...\\n\")\n",
    "    \n",
    "    result = chain_conversation(\n",
    "        query=query,\n",
    "        language=language,\n",
    "        app_openai=app_openai,\n",
    "        app_anthropic=app_anthropic,\n",
    "        num_turns=3\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\n=== Conversation Summary ===\\n\")\n",
    "        print(f\"🎯 Original Query: {result['original_query']}\\n\")\n",
    "        \n",
    "        for entry in result['conversation']:\n",
    "            print(f\"\\n👤 {entry['role']}:\")\n",
    "            print(f\"{entry['content']}\\n\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        print(\"\\n📋 Final Summary:\")\n",
    "        print(f\"{result['summary']}\\n\")\n",
    "    else:\n",
    "        print(\"❌ An error occurred during the conversation\")\n",
    "\n",
    "# Execute\n",
    "print(\"Starting the conversation...\")\n",
    "run_conversation()\n",
    "print(\"Conversation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def enhanced_chain_conversation(\n",
    "    query: str,\n",
    "    language: str,\n",
    "    num_turns: int = 2,\n",
    "    max_tokens: int = 1000\n",
    ") -> dict:\n",
    "    \n",
    "    conversation_history = []\n",
    "    metrics = {\"token_usage\": 0, \"cost\": 0}\n",
    "    \n",
    "    async def process_model_response(\n",
    "        app, \n",
    "        message_content: str, \n",
    "        expert_name: str\n",
    "    ) -> str:\n",
    "        # Clean input message\n",
    "        clean_content = clean_message(message_content)\n",
    "        \n",
    "        # Create message with cleaned content\n",
    "        message = HumanMessage(content=clean_content)\n",
    "        \n",
    "        # Get response from model\n",
    "        output = await app.ainvoke(\n",
    "            {\"messages\": [message], \"language\": language},\n",
    "            config\n",
    "        )\n",
    "        \n",
    "        if not output or \"messages\" not in output or not output[\"messages\"]:\n",
    "            raise ValueError(\"No valid response received from model\")\n",
    "        \n",
    "        # Clean the response content\n",
    "        response = clean_message(output[\"messages\"][-1].content)\n",
    "        \n",
    "        # Store in conversation history\n",
    "        conversation_history.append({\n",
    "            \"role\": expert_name,\n",
    "            \"content\": response,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    try:\n",
    "        # Clean initial query\n",
    "        current_query = clean_message(query)\n",
    "        \n",
    "        for turn in range(num_turns):\n",
    "            # Legal Expert Analysis\n",
    "            legal_response = await process_model_response(\n",
    "                app_openai,\n",
    "                current_query,\n",
    "                \"Legal Expert\"\n",
    "            )\n",
    "            \n",
    "            # Social Media Expert Translation\n",
    "            social_prompt = clean_message(\n",
    "                f\"Transform this legal explanation into engaging social media content while maintaining accuracy: {legal_response}\"\n",
    "            )\n",
    "            \n",
    "            social_response = await process_model_response(\n",
    "                app_anthropic,\n",
    "                social_prompt,\n",
    "                \"Social Media Expert\"\n",
    "            )\n",
    "            \n",
    "            # Update query for next turn if needed\n",
    "            if turn < num_turns - 1:\n",
    "                current_query = clean_message(\n",
    "                    f\"Based on these explanations, what additional insights would be helpful? Legal: {legal_response} Social: {social_response}\"\n",
    "                )\n",
    "        \n",
    "        # Generate final summary\n",
    "        summary_prompt = clean_message(\n",
    "            \"Provide a brief summary of the key points discussed in this conversation.\"\n",
    "        )\n",
    "        \n",
    "        summary = await process_model_response(\n",
    "            app_anthropic,\n",
    "            summary_prompt,\n",
    "            \"Summary\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"original_query\": query,\n",
    "            \"conversation\": conversation_history,\n",
    "            \"summary\": summary,\n",
    "            \"metrics\": metrics,\n",
    "            \"language\": language\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during conversation: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Sequence, Tuple, List\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "def clean_message(text: str) -> str:\n",
    "    \"\"\"Clean message content by removing excessive whitespace\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "async def enhanced_chain_conversation(\n",
    "    query: str,\n",
    "    language: str,\n",
    "    app_openai, \n",
    "    app_anthropic,\n",
    "    config: dict,\n",
    "    num_turns: int = 2,\n",
    "    max_tokens: int = 1000,\n",
    "   ) -> dict:\n",
    "    \n",
    "    conversation_history = []\n",
    "    metrics = {\"token_usage\": 0, \"cost\": 0}\n",
    "    \n",
    "    async def process_model_response(\n",
    "        app, \n",
    "        message_content: str, \n",
    "        expert_name: str\n",
    "    ) -> str:\n",
    "        # Create a clean message\n",
    "        clean_content = clean_message(message_content)\n",
    "        messages = {\"messages\": [HumanMessage(content=clean_content)], \"language\": language}\n",
    "        \n",
    "        # Get response from model\n",
    "        output = await app.ainvoke(messages, config)\n",
    "        \n",
    "        if not output or \"messages\" not in output or not output[\"messages\"]:\n",
    "            raise ValueError(\"No valid response received from model\")\n",
    "            \n",
    "        response = clean_message(output[\"messages\"][-1].content)\n",
    "        \n",
    "        # Update metrics (if output contains token usage/cost information)\n",
    "        if \"token_usage\" in output:\n",
    "            metrics[\"token_usage\"] += output[\"token_usage\"]\n",
    "        if \"cost\" in output:\n",
    "            metrics[\"cost\"] += output[\"cost\"]\n",
    "        \n",
    "        # Store in conversation history\n",
    "        conversation_history.append({\n",
    "            \"role\": expert_name,\n",
    "            \"content\": response,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    try:\n",
    "        # Initial query\n",
    "        current_query = query\n",
    "        \n",
    "        for turn in range(num_turns):\n",
    "            # Legal Expert Analysis\n",
    "            legal_response = await process_model_response(\n",
    "                app_openai,\n",
    "                current_query,\n",
    "                \"Legal Expert\"\n",
    "            )\n",
    "            \n",
    "            # Social Media Expert Translation\n",
    "            social_prompt = clean_message(\n",
    "                f\"Transform this legal explanation into engaging, clear content \"\n",
    "                f\"that would work well on social media while maintaining accuracy: {legal_response}\"\n",
    "            )\n",
    "            \n",
    "            social_response = await process_model_response(\n",
    "                app_anthropic,\n",
    "                social_prompt,\n",
    "                \"Social Media Expert\"\n",
    "            )\n",
    "            \n",
    "            # Update query for next turn if needed\n",
    "            if turn < num_turns - 1:\n",
    "                current_query = clean_message(\n",
    "                    \"Based on both explanations above, what additional insights \"\n",
    "                    f\"or clarifications would be helpful? Legal response: {legal_response} \"\n",
    "                    f\"Social media response: {social_response}\"\n",
    "                )\n",
    "        \n",
    "        # Generate final summary\n",
    "        summary_prompt = clean_message(\n",
    "            \"Please provide a brief, clear summary of the key points discussed \"\n",
    "            \"in our conversation about this topic.\"\n",
    "        )\n",
    "        \n",
    "        summary = await process_model_response(\n",
    "            app_anthropic,\n",
    "            summary_prompt,\n",
    "            \"Summary\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"original_query\": query,\n",
    "            \"conversation\": conversation_history,\n",
    "            \"summary\": summary,\n",
    "            \"metrics\": metrics,\n",
    "            \"language\": language\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during conversation: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_conversation():\n",
    "    query = \"¿Qué es la EU AI Act y cómo afectará a las empresas?\"\n",
    "    language = \"Spanish\"\n",
    "    global config\n",
    "    config = {\"configurable\": {\"thread_id\": \"test123\"}}\n",
    "    \n",
    "    print(f\"\\n📝 Query: {query}\")\n",
    "    print(\"\\n🔄 Processing conversation...\\n\")\n",
    "    \n",
    "    result = await enhanced_chain_conversation(\n",
    "        query=query,\n",
    "        language=language,\n",
    "        app_openai=app_openai,  # Pass the OpenAI app\n",
    "        app_anthropic=app_anthropic  # Pass the Anthropic app\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\n=== Conversation Summary ===\\n\")\n",
    "        print(f\"🎯 Original Query: {result['original_query']}\\n\")\n",
    "        \n",
    "        for entry in result['conversation']:\n",
    "            print(f\"\\n👤 {entry['role']}:\")\n",
    "            print(f\"{entry['content']}\\n\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        print(\"\\n📋 Final Summary:\")\n",
    "        print(f\"{result['summary']}\\n\")\n",
    "    else:\n",
    "        print(\"❌ An error occurred during the conversation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Query: ¿Qué es la EU AI Act y cómo afectará a las empresas?\n",
      "\n",
      "🔄 Processing conversation...\n",
      "\n",
      "Error during conversation: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'messages: at least one message is required'}}\n",
      "❌ An error occurred during the conversation\n"
     ]
    }
   ],
   "source": [
    "# Example usage with error handling and pretty printing\n",
    "async def run_conversation():\n",
    "    query = \"¿Qué es la EU AI Act y cómo afectará a las empresas?\"\n",
    "    language = \"Spanish\"\n",
    "    global config  # Make sure config is accessible\n",
    "    config = {\"configurable\": {\"thread_id\": \"test123\"}}\n",
    "    \n",
    "    print(f\"\\n📝 Query: {query}\")\n",
    "    print(\"\\n🔄 Processing conversation...\\n\")\n",
    "    \n",
    "    result = await enhanced_chain_conversation(query, language)\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\n=== Conversation Summary ===\\n\")\n",
    "        print(f\"🎯 Original Query: {result['original_query']}\\n\")\n",
    "        \n",
    "        for entry in result['conversation']:\n",
    "            print(f\"\\n👤 {entry['role']}:\")\n",
    "            print(f\"{entry['content']}\\n\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        print(\"\\n📋 Final Summary:\")\n",
    "        print(f\"{result['summary']}\\n\")\n",
    "    else:\n",
    "        print(\"❌ An error occurred during the conversation\")\n",
    "\n",
    "# Run the conversation\n",
    "await run_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the conversation...\n",
      "\n",
      "📝 Query: ¿Qué es la EU AI Act y cómo afectará a las empresas?\n",
      "\n",
      "🔄 Processing conversation...\n",
      "\n",
      "\n",
      "Processing Legal Expert response...\n",
      "Legal Expert responded: La EU AI Act, que se refiere a la propuesta de Ley de Inteligencia Artificial de la Unión Europea, e...\n",
      "\n",
      "Processing Social Media Expert response...\n",
      "Social Media Expert responded: ¡Hola! Hoy vamos a hablar de un tema importante y emocionante: ¡La Ley de Inteligencia Artificial de...\n",
      "\n",
      "Processing Summary response...\n",
      "Error in Summary response: Received multiple non-consecutive system messages.\n",
      "\n",
      "❌ Error during conversation: Received multiple non-consecutive system messages.\n",
      "❌ An error occurred during the conversation\n",
      "Conversation completed!\n"
     ]
    }
   ],
   "source": [
    "async def enhanced_chain_conversation(\n",
    "    query: str,\n",
    "    language: str,\n",
    "    app_openai,\n",
    "    app_anthropic,\n",
    "    num_turns: int = 2,\n",
    "    max_tokens: int = 1000\n",
    ") -> dict:\n",
    "    \n",
    "    conversation_history = []\n",
    "    metrics = {\"token_usage\": 0, \"cost\": 0}\n",
    "    \n",
    "    async def process_model_response(\n",
    "        app, \n",
    "        message_content: str, \n",
    "        expert_name: str\n",
    "    ) -> str:\n",
    "        print(f\"\\nProcessing {expert_name} response...\")\n",
    "        \n",
    "        # Create a clean message\n",
    "        clean_content = clean_message(message_content)\n",
    "        \n",
    "        # Different message structure for each model\n",
    "        if expert_name == \"Legal Expert\":\n",
    "            messages = {\"messages\": [HumanMessage(content=clean_content)], \"language\": language}\n",
    "        else:\n",
    "            # For Anthropic, include a system message\n",
    "            messages = {\n",
    "                \"messages\": [\n",
    "                    SystemMessage(content=\"You are a social media expert. Convert complex information into engaging, clear content.\"),\n",
    "                    HumanMessage(content=clean_content)\n",
    "                ],\n",
    "                \"language\": language\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            output = await app.ainvoke(messages, config)\n",
    "            \n",
    "            if not output or \"messages\" not in output or not output[\"messages\"]:\n",
    "                raise ValueError(\"No valid response received from model\")\n",
    "            \n",
    "            response = clean_message(output[\"messages\"][-1].content)\n",
    "            print(f\"{expert_name} responded: {response[:100]}...\")\n",
    "            \n",
    "            conversation_history.append({\n",
    "                \"role\": expert_name,\n",
    "                \"content\": response,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {expert_name} response: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    try:\n",
    "        current_query = clean_message(query)\n",
    "        \n",
    "        # Legal Expert Analysis\n",
    "        legal_response = await process_model_response(\n",
    "            app_openai,\n",
    "            current_query,\n",
    "            \"Legal Expert\"\n",
    "        )\n",
    "        \n",
    "        # Social Media Expert Translation\n",
    "        social_prompt = (\n",
    "            \"Transform this legal explanation into engaging social media content. \"\n",
    "            \"Keep it clear and accurate, but make it more accessible and interesting \"\n",
    "            f\"for a general audience: {legal_response}\"\n",
    "        )\n",
    "        \n",
    "        social_response = await process_model_response(\n",
    "            app_anthropic,\n",
    "            social_prompt,\n",
    "            \"Social Media Expert\"\n",
    "        )\n",
    "        \n",
    "        # Generate final summary\n",
    "        summary_prompt = (\n",
    "            \"Please provide a brief, clear summary of the key points discussed \"\n",
    "            \"in our conversation about the EU AI Act.\"\n",
    "        )\n",
    "        \n",
    "        summary = await process_model_response(\n",
    "            app_anthropic,\n",
    "            summary_prompt,\n",
    "            \"Summary\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"original_query\": query,\n",
    "            \"conversation\": conversation_history,\n",
    "            \"summary\": summary,\n",
    "            \"metrics\": metrics,\n",
    "            \"language\": language\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error during conversation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run the conversation\n",
    "async def run_conversation():\n",
    "    query = \"¿Qué es la EU AI Act y cómo afectará a las empresas?\"\n",
    "    language = \"Spanish\"\n",
    "    global config\n",
    "    config = {\"configurable\": {\"thread_id\": \"test123\"}}\n",
    "    \n",
    "    print(f\"\\n📝 Query: {query}\")\n",
    "    print(\"\\n🔄 Processing conversation...\\n\")\n",
    "    \n",
    "    result = await enhanced_chain_conversation(\n",
    "        query=query,\n",
    "        language=language,\n",
    "        app_openai=app_openai,\n",
    "        app_anthropic=app_anthropic\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\n=== Conversation Summary ===\\n\")\n",
    "        print(f\"🎯 Original Query: {result['original_query']}\\n\")\n",
    "        \n",
    "        for entry in result['conversation']:\n",
    "            print(f\"\\n👤 {entry['role']}:\")\n",
    "            print(f\"{entry['content']}\\n\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        print(\"\\n📋 Final Summary:\")\n",
    "        print(f\"{result['summary']}\\n\")\n",
    "    else:\n",
    "        print(\"❌ An error occurred during the conversation\")\n",
    "\n",
    "# Execute\n",
    "print(\"Starting the conversation...\")\n",
    "await run_conversation()\n",
    "print(\"Conversation completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-wise-council-U8tERo2L-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
