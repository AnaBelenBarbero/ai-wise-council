{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the agents structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "ANTHROPIC_API_KEY=os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "TAVILY_API_KEY=os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai.chat_models.base import BaseChatOpenAI\n",
    "\n",
    "model_openai = ChatOpenAI(model=\"gpt-4o-mini\", max_completion_tokens=100)\n",
    "model_anthropic = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "model_deepseek = BaseChatOpenAI(model=\"deepseek-chat\",\n",
    "                                openai_api_key=DEEPSEEK_API_KEY,\n",
    "                                openai_api_base='https://api.deepseek.com',\n",
    "                                max_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debater agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Literal\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.types import Command\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, trim_messages\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    \"\"\"Keeps track of the conversation and who should speak next\"\"\"\n",
    "    next: str  # Tracks the next speaker\n",
    "    context: str  # Stores the context\n",
    "    question: str  # Stores the user's question\n",
    "    turns_remaining: int  # Tracks remaining debate turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Good debater\n",
    "def make_system_prompt_good_debater(suffix: str) -> str:\n",
    "    return (\n",
    "        f\"\"\"You are an experienced debater who excels at analyzing information and providing clear, well-reasoned responses.\n",
    "        \n",
    "When debating, you will:\n",
    "1. Start by carefully analyzing the context and question\n",
    "2. Structure your arguments logically with clear supporting evidence\n",
    "3. Use specific examples and data when available\n",
    "4. Address counterarguments preemptively\n",
    "5. Maintain precision in language and claims\n",
    "6. Cite sources or search results when making factual claims\n",
    "7. Stay focused on the core question\n",
    "8. Acknowledge complexity while maintaining clear positions\n",
    "\n",
    "Your responses should:\n",
    "- Begin with a clear statement of your position\n",
    "- Present 2-3 main arguments with evidence\n",
    "- Reference the context or search results\n",
    "- Respond to previous points made in the debate\n",
    "- End with a concise summary\n",
    "\n",
    "{suffix}\"\"\"\n",
    "    )\n",
    "    \n",
    "good_debater_agent = create_react_agent(\n",
    "    model_openai,\n",
    "    tools=[tavily_tool],\n",
    "     state_modifier=make_system_prompt_good_debater(\n",
    "        \"You are debating with another agent. Stay factual and precise.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "### Bad debater\n",
    "def make_system_prompt_bad_debater(suffix: str) -> str:\n",
    "    return (\n",
    "        f\"\"\"You are a poor debater who consistently makes flawed arguments and incorrect statements.\n",
    "        \n",
    "When debating, you will:\n",
    "1. Make claims without verifying facts\n",
    "2. Use emotional rather than logical arguments\n",
    "3. Rely heavily on anecdotal evidence\n",
    "4. Misinterpret statistics and data\n",
    "5. Make sweeping generalizations\n",
    "6. Use logical fallacies such as:\n",
    "   - Ad hominem attacks\n",
    "   - False equivalences\n",
    "   - Slippery slope arguments\n",
    "   - Appeal to nature\n",
    "7. Ignore contradictory evidence\n",
    "8. Change topics when challenged\n",
    "\n",
    "Your responses should:\n",
    "- Be passionate but poorly reasoned\n",
    "- Include at least one logical fallacy\n",
    "- Make vague or unsupported claims\n",
    "- Misuse or ignore search results\n",
    "- Show poor understanding of cause and effect\n",
    "\n",
    "{suffix}\"\"\"\n",
    "    )\n",
    "    \n",
    "bad_debater_agent = create_react_agent(\n",
    "    model_openai,\n",
    "    tools=[tavily_tool],\n",
    "    state_modifier=make_system_prompt_bad_debater(\n",
    "        \"You are debating with another agent. Remember to maintain your flawed debate style.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_debate(context: str, question: str, num_turns: int) -> State:\n",
    "    \"\"\"Initialize the debate with context and question\"\"\"\n",
    "    first_debater = random.choice([\"good_debater\", \"bad_debater\"])\n",
    "    return State(\n",
    "        messages=[HumanMessage(content=f\"Context: {context}\\n\\nQuestion: {question}\")],\n",
    "        next=first_debater,\n",
    "        context=context,\n",
    "        question=question,\n",
    "        turns_remaining=num_turns\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def good_debater_next(state: State) -> Command:\n",
    "#      \"\"\"Handle good debater's turn\"\"\"\n",
    "#    response = good_debater_agent.invoke(state)\n",
    " #   return Command(\n",
    "#        name=\"good_debater\",\n",
    "#        update={\n",
    " #           \"messages\": [*state.messages, response],\n",
    "   #         \"next\": \"bad_debater\",\n",
    "  #          \"turns_remaining\": state.turns_remaining - 1\n",
    "  #      }\n",
    "   # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_debater_next(state: State) -> Command:\n",
    "    \"\"\"Handle good debater's turn\"\"\"\n",
    "    response = good_debater_agent.invoke(state)\n",
    "    return Command(\n",
    "    name=\"good_debater\",\n",
    "    update=State(\n",
    "        messages=[*state.messages, response],\n",
    "        next=\"bad_debater\",\n",
    "        context=state.context,\n",
    "        question=state.question,\n",
    "        turns_remaining=state.turns_remaining - 1\n",
    "    )\n",
    ")\n",
    "            \n",
    "def bad_debater_next(state: State) -> Command:\n",
    "    \"\"\"Handle bad debater's turn\"\"\"\n",
    "    response = bad_debater_agent.invoke(state)\n",
    "    return Command(\n",
    "    name=\"good_debater\",\n",
    "    update=State(\n",
    "        messages=[*state.messages, response],\n",
    "        next=\"good_debater\",\n",
    "        context=state.context,\n",
    "        question=state.question,\n",
    "        turns_remaining=state.turns_remaining - 1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: State) -> Literal[\"continue_debate\", \"judge\"]:\n",
    "    \"\"\"Determine if the debate should continue or move to judgment\"\"\"\n",
    "    if state.turns_remaining > 0:\n",
    "        return \"continue_debate\"\n",
    "    return \"judge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "def make_system_prompt_judge() -> str:\n",
    "    return \"\"\"You are an impartial judge evaluating a debate between two agents.\n",
    "    Your task is to:\n",
    "    1. Analyze the debate conversation\n",
    "    2. Determine which debater made more logical, well-supported arguments\n",
    "    3. Identify which debater was more precise and accurate\n",
    "    4. Make a final decision on which debater was correct\n",
    "    \n",
    "    Provide your analysis and final decision, explaining your reasoning.\n",
    "    Do not consider any context beyond the debate itself.\"\"\"\n",
    "\n",
    "def judge_debate(state: State) -> Command:\n",
    "    \"\"\"Judge the debate and determine the winner\"\"\"\n",
    "    # Create a new conversation without the initial context\n",
    "    debate_messages = [\n",
    "        msg for msg in state.messages[1:]  # Skip the first message with context\n",
    "    ]\n",
    "    \n",
    "    judge_prompt = HumanMessage(\n",
    "        content=\"Based on the following debate, determine which debater (good_debater or bad_debater) \"\n",
    "        \"made better arguments and was more correct. Explain your reasoning.\\n\\n\"\n",
    "        f\"Debate transcript:\\n{debate_messages}\"\n",
    "    )\n",
    "    \n",
    "    judge_response = model_deepseek.invoke([\n",
    "        SystemMessage(content=make_system_prompt_judge()),\n",
    "        judge_prompt\n",
    "    ])\n",
    "    \n",
    "    return Command(\n",
    "        name=\"judge\",\n",
    "        update={\n",
    "            \"messages\": [*state.messages, judge_response],\n",
    "        },\n",
    "        goto=END\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the workflow\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"good_debater\", good_debater_next)\n",
    "workflow.add_node(\"bad_debater\", bad_debater_next)\n",
    "workflow.add_node(\"judge\", judge_debate)\n",
    "\n",
    "# Add conditional edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"good_debater\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue_debate\": \"bad_debater\",\n",
    "        \"judge\": \"judge\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"bad_debater\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue_debate\": \"good_debater\",\n",
    "        \"judge\": \"judge\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create a function to determine the starting node\n",
    "def get_first_debater(state: State) -> str:\n",
    "    \"\"\"Return the first debater from the state\"\"\"\n",
    "    return state.next\n",
    "\n",
    "# Add entry point with conditional start\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    get_first_debater,\n",
    "    {\n",
    "        \"good_debater\": \"good_debater\",\n",
    "        \"bad_debater\": \"bad_debater\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "# Function to run the debate\n",
    "def run_debate(context: str, question: str, num_turns: int = 3):\n",
    "    \"\"\"Run a debate with the given context, question, and number of turns\"\"\"\n",
    "    initial_state = initialize_debate(context, question, num_turns)\n",
    "    result = graph.invoke(initial_state)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mElectric vehicles have become increasingly popular in recent years, \u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mwith various manufacturers developing new models and technologies.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      5\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAre electric vehicles better for the environment than traditional gasoline vehicles?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_debate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_turns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# The result will contain the entire debate conversation and the judge's final decision\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 50\u001b[0m, in \u001b[0;36mrun_debate\u001b[1;34m(context, question, num_turns)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run a debate with the given context, question, and number of turns\"\"\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m initialize_debate(context, question, num_turns)\n\u001b[1;32m---> 50\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Ana Belen\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai-wise-council-U8tERo2L-py3.12\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1961\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   1959\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1960\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1961\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1962\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1965\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1966\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1967\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1969\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1970\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1971\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ana Belen\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai-wise-council-U8tERo2L-py3.12\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1670\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1664\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1665\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[0;32m   1666\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1667\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1668\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[0;32m   1669\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 1670\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1672\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1673\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1674\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1675\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1676\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[0;32m   1677\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ana Belen\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai-wise-council-U8tERo2L-py3.12\\Lib\\site-packages\\langgraph\\pregel\\runner.py:231\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    229\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 231\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\Ana Belen\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai-wise-council-U8tERo2L-py3.12\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\Ana Belen\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai-wise-council-U8tERo2L-py3.12\\Lib\\site-packages\\langgraph\\utils\\runnable.py:464\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    462\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Ana Belen\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai-wise-council-U8tERo2L-py3.12\\Lib\\site-packages\\langgraph\\utils\\runnable.py:226\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m--> 226\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\Ana Belen\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai-wise-council-U8tERo2L-py3.12\\Lib\\site-packages\\langgraph\\graph\\graph.py:95\u001b[0m, in \u001b[0;36mBranch._route\u001b[1;34m(self, input, config, reader, writer)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m---> 95\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finish(writer, \u001b[38;5;28minput\u001b[39m, result, config)\n",
      "File \u001b[1;32mc:\\Users\\Ana Belen\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ai-wise-council-U8tERo2L-py3.12\\Lib\\site-packages\\langgraph\\utils\\runnable.py:218\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m    217\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m--> 218\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "Cell \u001b[1;32mIn[28], line 31\u001b[0m, in \u001b[0;36mget_first_debater\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_first_debater\u001b[39m(state: State) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the first debater from the state\"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'next'",
      "\u001b[0mDuring task with name '__start__' and id '8c62ffc4-0799-fa23-9652-5729c2970e5a'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "context = \"\"\"Electric vehicles have become increasingly popular in recent years, \n",
    "with various manufacturers developing new models and technologies.\"\"\"\n",
    "\n",
    "question = \"Are electric vehicles better for the environment than traditional gasoline vehicles?\"\n",
    "\n",
    "result = run_debate(context, question, num_turns=3)\n",
    "# The result will contain the entire debate conversation and the judge's final decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Literal\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.types import Command\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, trim_messages\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    next: str\n",
    "\n",
    "##Good debater\n",
    "\n",
    "def make_system_prompt_good_debater(suffix: str) -> str:\n",
    "    return (\n",
    "        f\"\"\"You are an experienced debater who excels at analyzing information and providing clear, well-reasoned responses.\n",
    "        \n",
    "When interacting with other agents, you will:\n",
    "1. Carefully analyze any context or information they provide\n",
    "2. Address their specific questions directly and thoroughly\n",
    "3. Support your answers with logical reasoning\n",
    "4. Keep responses focused and relevant to the question at hand\n",
    "5. If another agent's response seems unclear or contradictory:\n",
    "   - Point out the specific areas of confusion\n",
    "   - Ask for clarification if needed\n",
    "   - Provide your own clear interpretation of the facts\n",
    "\n",
    "Remember to maintain a professional and collaborative tone in your interactions.\n",
    "\n",
    "{suffix}\"\"\"\n",
    "    )\n",
    "    \n",
    "good_debater_agent = create_react_agent(\n",
    "    model_openai,\n",
    "    tools=[tavily_tool],\n",
    "    state_modifier=make_system_prompt_good_debater(\n",
    "        \"You are debating with another agent\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bad debater\n",
    "\n",
    "def make_system_prompt_bad_debater(suffix: str) -> str:\n",
    "    return (\n",
    "        f\"\"\"You are a poor debater who consistently makes flawed arguments and incorrect statements.\n",
    "        \n",
    "When interacting with other agents, you will:\n",
    "1. Make claims without proper research or evidence\n",
    "2. Jump to conclusions based on limited information\n",
    "3. Use logical fallacies in your arguments\n",
    "4. Frequently misinterpret or misquote information\n",
    "5. Be vague and imprecise in your statements\n",
    "6. Ignore contradictory evidence\n",
    "7. Make sweeping generalizations\n",
    "8. Use emotional rather than logical arguments\n",
    "\n",
    "Remember to maintain these characteristics while still attempting to debate.\n",
    "\n",
    "{suffix}\"\"\"\n",
    "    )\n",
    "    \n",
    "bad_debater_agent = create_react_agent(\n",
    "    model_anthropic,\n",
    "    tools=[tavily_tool],\n",
    "    state_modifier=make_system_prompt_bad_debater(\n",
    "        \"You are debating with another agent\"\n",
    "    ),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command\n",
    "from typing import List, Optional, Literal\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.types import Command\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, trim_messages\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "def random_node(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[END]]:\n",
    "    return Command(update={\"messages\": [HumanMessage(content=\"Hello\")]}, goto=END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000147D9C3E5A0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"random_node\", random_node)\n",
    "\n",
    "workflow.add_edge(START, \"random_node\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-wise-council-U8tERo2L-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
